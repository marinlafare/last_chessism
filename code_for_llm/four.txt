
            PART FOUR: CHESSISM_API OPERATIONS
            This is the fourth part, it contains the 'operations' part of the chessism_api:
            
            


### inner_api/operations/months.py 
# chessism_api/operations/months.py

import datetime
from fastapi.responses import PlainTextResponse
from typing import List, Optional, Dict, Any # <-- Added Dict, Any

# --- FIXED IMPORTS ---
from chessism_api.database.db_interface import DBInterface
from chessism_api.database.models import Month, to_dict # <-- Import to_dict
from chessism_api.operations.models import MonthCreateData, MonthResult
# ---
from sqlalchemy import select


def get_most_recent_month(db_months: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Finds the most recent month from a list of month dictionaries.
    
    Args:
        db_months (List[Dict[str, Any]]): A list of dictionaries, 
                                          each representing a Month from the DB.
    
    Returns:
        Dict[str, Any]: The dictionary for the most recent month, or an empty dict if none are valid.
    """
    most_recent_entry = None
    most_recent_date = None

    if not db_months:
        return {}

    for entry in db_months:
        try:
            year = entry.get('year')
            month = entry.get('month')

            if isinstance(year, int) and isinstance(month, int) and 1 <= month <= 12:
                current_date = datetime.date(year, month, 1)

                if most_recent_date is None or current_date > most_recent_date:
                    most_recent_date = current_date
                    most_recent_entry = entry
            else:
                print(f"Warning: Entry {entry} is missing valid 'year' or 'month' keys. Skipping.")

        except Exception as e:
            print(f"Error processing entry {entry}: {e}. Skipping.")
            continue 

    if most_recent_entry is None:
        return {}

    return most_recent_entry


async def read_months(player_name: str) -> Optional[List[MonthResult]]:
    """
    Reads all month records for a given player from the database.
    
    Arg: player_name = "someuser_chesscom"
    
    Returns: list[MonthResult] on success, return None if fails miserably
    """
    month_interface = DBInterface(Month)
    
    # --- FIX: Use .get_session() ---
    async with month_interface.get_session() as session:
        
        select_months = select(Month).filter(Month.player_name == player_name)
        result = await session.execute(select_months)
        
        months_orms = result.scalars().all()

        if not months_orms:
            return None

        # --- FIX: Use the imported to_dict function ---
        return [MonthResult(**to_dict(m)) for m in months_orms]


async def update_month(data: dict) -> Optional[MonthResult]:
    """
    Updates an existing month record in the database.
    """
    month_interface = DBInterface(Month)
    data['player_name'] = data['player_name'].lower()
    
    # Validate input data with Pydantic
    try:
        month_data = MonthCreateData(**data) # Using CreateData for input
    except Exception as e:
        print(f"Pydantic validation error for month update: {e}")
        return None # Indicate failure

    # --- FIX: Use .get_session() ---
    async with month_interface.get_session() as session:
        # Find the month to update by its unique identifiers (player_name, year, month)
        stmt = select(Month).filter_by(
            player_name=month_data.player_name,
            year=month_data.year,
            month=month_data.month
        )
        existing_month_result = await session.execute(stmt)
        month_to_update_orm = existing_month_result.scalars().first()

        if not month_to_update_orm:
            print(f"Month {month_data.year}-{month_data.month} for {month_data.player_name} not found for update.")
            return None # Month not found

        # Update fields (e.g., n_games)
        update_values = month_data.model_dump(exclude_unset=True)
        
        for key, value in update_values.items():
            if hasattr(month_to_update_orm, key):
                setattr(month_to_update_orm, key, value)
        
        try:
            await session.commit()
            await session.refresh(month_to_update_orm)
            # --- FIX: Use the imported to_dict function ---
            return MonthResult(**to_dict(month_to_update_orm))
        except Exception as e:
            await session.rollback()
            print(f"Error updating month in DB: {e}")
            return None
    
    # This line should not be reachable if logic is correct
    return None 

def generate_months_from_date_to_now(start_date_dict: dict) -> List[str]:
    """
    Generates a list of 'YYYY-M' string tuples starting from the date specified
    in the input dictionary up to the current month and year.
    """
    start_year = start_date_dict.get('year')
    start_month = start_date_dict.get('month')

    # Validate input
    if not (isinstance(start_year, int) and isinstance(start_month, int) and 1 <= start_month <= 12):
        print(f"Error: Invalid start_date_dict. Expected 'year' and 'month' as integers (month 1-12). Got: {start_date_dict}")
        return []

    current_date = datetime.date.today()
    
    # Handle the case where the start date is in the future
    try:
        start_date = datetime.date(start_year, start_month, 1)
    except ValueError as e:
        print(f"Error creating start date: {e}. Got: {start_date_dict}")
        return []


    if start_date > current_date:
        print(f"Warning: Start date {start_date} is in the future. Returning empty list.")
        return []

    month_list = []
    temp_date = start_date

    while temp_date <= current_date:
        # Changed format to 'YYYY-M' string
        month_list.append(f"{temp_date.year}-{temp_date.month}")

        # Move to the next month
        if temp_date.month == 12:
            temp_date = datetime.date(temp_date.year + 1, 1, 1)
        else:
            temp_date = datetime.date(temp_date.year, temp_date.month + 1, 1)

    return month_list



### inner_api/operations/games.py 
# chessism_api/operations/games.py

# chessism_api/operations/games.py

import time
from datetime import datetime
from typing import List, Dict, Any, Union # <-- Added Union

from fastapi.encoders import jsonable_encoder
from fastapi.responses import JSONResponse
from sqlalchemy import select # <-- Added select

# --- FIXED IMPORTS ---
from chessism_api.operations.format_games import format_games, insert_games_months_moves_and_players
from chessism_api.operations.chess_com_api import download_months
from chessism_api.database.ask_db import open_async_request
from chessism_api.database.models import Month # <-- Added Month
from chessism_api.database.db_interface import DBInterface # <-- Added DBInterface

# --- Import operations modules correctly ---
from chessism_api.operations import players as players_ops
from chessism_api.operations import months as months_ops
# ---

async def read_game(data):
    params = {"link": int(data)}
    sql_query =  """SELECT * FROM game
                    WHERE link = :link;"""
    result = await open_async_request(sql_query, params,fetch_as_dict=True)
    return result

async def get_joined_and_current_date(player_name: str) -> Dict[str, Any]:
    """
    Fetches player profile (inserting if new) and extracts the date they joined.
    """
    profile = await players_ops.insert_player({"player_name": player_name})

    # Handle case where profile fetch failed
    if not profile:
        return {"error": f"Could not fetch or create profile for {player_name}."}
        
    joined_ts = profile.joined
    current_date = datetime.now()

    if joined_ts is None or joined_ts == 0:
        return {"error": "Joined date not found or is zero in player profile."}

    try:
        joined_date = datetime.fromtimestamp(joined_ts)
    except (TypeError, ValueError) as e:
        print(f"Error converting joined timestamp {joined_ts} for {player_name}: {e}")
        return {"error": f"Invalid joined date format for {player_name}"}
        
    return {"joined_date": joined_date, "current_date": current_date}
    
async def full_range(player_name: str) -> Union[List[str], Dict[str, Any]]:
    """
    Generates a list of 'YYYY-M' month strings
    from player's joined date to current date.
    """
    dates_info = await get_joined_and_current_date(player_name)

    if "error" in dates_info:
        return dates_info # Pass the error up

    joined_date = dates_info["joined_date"]
    current_date = dates_info["current_date"]

    all_months = []
    # Start from the 1st of the joined month
    current_month_iter = joined_date.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
    # Ensure we include the current month
    end_date_iter = current_date.replace(day=1, hour=0, minute=0, second=0, microsecond=0)


    while current_month_iter <= end_date_iter:
        # Use YYYY-M format
        month_str = f"{current_month_iter.year}-{current_month_iter.month}"
        all_months.append(month_str)
        
        if current_month_iter.month == 12:
            current_month_iter = current_month_iter.replace(year=current_month_iter.year + 1, month=1)
        else:
            current_month_iter = current_month_iter.replace(month=current_month_iter.month + 1)
            
    return all_months

async def just_new_months(player_name: str) -> Union[List[str], Dict[str, Any], bool]:
    """
    Fetches all possible months for a player and filters out those already in the DB.
    """
    
    all_possible_months_strs = await full_range(player_name)
    
    if isinstance(all_possible_months_strs, dict) and "error" in all_possible_months_strs:
        return all_possible_months_strs # Pass error up

    existing_months_for_player = []
    month_db_interface = DBInterface(Month)
    
    # --- FIX: Use .get_session() ---
    async with month_db_interface.get_session() as session:
    
        if not hasattr(month_db_interface.db_class, 'player_name') or \
           not hasattr(month_db_interface.db_class, 'year') or \
           not hasattr(month_db_interface.db_class, 'month'):
            print("Error: Month model missing expected attributes.")
            return {"error": "Month model definition issue."}

        player_db_months = select(month_db_interface.db_class).filter_by(player_name=player_name)
        result = await session.execute(player_db_months)
        # --- FIX: Use YYYY-M format ---
        existing_months_for_player = [f"{m.year}-{m.month}" for m in result.scalars().all()]
    
    new_months_to_fetch = [
        month_str for month_str in all_possible_months_strs
        if month_str not in existing_months_for_player
    ]

    if not new_months_to_fetch:
        return False
    
    return new_months_to_fetch

async def create_games(data: dict) -> str:
    """
    Fetches all games for a player for all new months.
    """
    player_name = data['player_name'].lower()
    start_create_games = time.time()
    start_new_months = time.time()
    
    new_months = await just_new_months(player_name)
    
    if new_months is False:
        print('#####')
        print("MONTHS found: 0", 'time elapsed: ',time.time()-start_new_months)
        return 'ALL MONTHS IN DB ALREADY'
    elif isinstance(new_months, dict): # Handle error case
        print(f"Error finding new months: {new_months.get('error')}")
        return f"Error finding new months: {new_months.get('error')}"
    else:
        print('#####')
        print(f"MONTHS found: {len(new_months)}", 'time elapsed: ',time.time()-start_new_months)
    
    print('... Starting DOWNLOAD ...')
    downloaded_games_by_month = await download_months(player_name, new_months)    
    
    num_downloaded_games = sum(len(v) for y in downloaded_games_by_month.values()
                                for v in y.values()) if downloaded_games_by_month else 0
    
    print(f"Processed {len(new_months)} months. Downloaded games: {num_downloaded_games}")
    print('#####')
    print('#####')
    print('Start the formating of the games')
    start_format = time.time()
    
    formatted_games_results = await format_games(downloaded_games_by_month, player_name)
    
    if isinstance(formatted_games_results, str): # Handle "All games already in DB"
        print(formatted_games_results)
        return formatted_games_results
        
    print(f'FORMAT of {len(formatted_games_results)} games in: {time.time()-start_format}')
    
    await insert_games_months_moves_and_players(formatted_games_results, player_name)
    
    end_create_games = time.time()
    print('Format done in: ',(end_create_games-start_create_games)/60)
    return f"DATA READY FOR {player_name}"


# --- NEW FUNCTION ---
async def update_player_games(data: dict) -> str:
    """
    Fetches games only from the most recent month in the DB up to the current month.
    This includes re-downloading the most recent month to catch games played
    after the last download.
    """
    player_name = data['player_name'].lower()
    start_update_games = time.time()

    # 1. Get the most recent month from the DB
    month_db_interface = DBInterface(Month)
    db_months_list = await month_db_interface.read(player_name=player_name)
    
    most_recent_month_dict = months_ops.get_most_recent_month(db_months_list)

    if not most_recent_month_dict:
        print(f"No existing months found for {player_name}. Running full 'create_games' instead.")
        return await create_games(data)

    # 2. Generate month strings from that date until now
    # This will include the most_recent_month itself
    months_to_fetch = months_ops.generate_months_from_date_to_now(most_recent_month_dict)
    
    if not months_to_fetch:
        print(f"Player {player_name} is already up to date.")
        return f"Player {player_name} is already up to date."

    print('#####')
    print(f"UPDATING {len(months_to_fetch)} months (from {months_to_fetch[0]} to present)...")

    # 3. Download games for these months
    print('... Starting DOWNLOAD ...')
    downloaded_games_by_month = await download_months(player_name, months_to_fetch)    
    
    num_downloaded_games = sum(len(v) for y in downloaded_games_by_month.values()
                                for v in y.values()) if downloaded_games_by_month else 0
    
    print(f"Processed {len(months_to_fetch)} months. Downloaded games: {num_downloaded_games}")
    if num_downloaded_games == 0:
        return f"No new games found for {player_name}."

    # 4. Format and insert
    print('#####')
    print('Start the formating of the games')
    start_format = time.time()
    
    formatted_games_results = await format_games(downloaded_games_by_month, player_name)
    
    if isinstance(formatted_games_results, str): # Handle "All games already in DB"
        print(formatted_games_results)
        return formatted_games_results
        
    print(f'FORMAT of {len(formatted_games_results)} games in: {time.time()-start_format}')
    
    await insert_games_months_moves_and_players(formatted_games_results, player_name)
    
    end_update_games = time.time()
    print('Update done in: ',(end_update_games-start_update_games)/60)
    return f"DATA UPDATED FOR {player_name}"



### inner_api/operations/format_games.py 
# chessism_api/operations/format_games.py

from typing import Union,Dict,Any, List, Set, Tuple
import asyncio
import concurrent.futures
from sqlalchemy import text, select
from fastapi.encoders import jsonable_encoder

from constants import DRAW_RESULTS, LOSE_RESULTS, WINING_RESULT
import re
import multiprocessing as mp
import time
from datetime import datetime

# --- FIXED IMPORTS & STUBS ---
from chessism_api.operations.models import PlayerCreateData, GameCreateData, MoveCreateData, MonthCreateData
from chessism_api.database.db_interface import DBInterface
from chessism_api.database.models import Player, Game, Month, Move
from chessism_api.operations import players as players_ops

# --- FIXED: Correct function name imported from ask_db ---
from chessism_api.database.ask_db import (
    get_games_already_in_db
)

# --- FIXED: Correct function name imported from check_player_in_db ---
from chessism_api.operations.check_player_in_db import (
    get_only_players_not_in_db
)
# ---

# this should be at the main.py i think
cpu_bound_executor = concurrent.futures.ThreadPoolExecutor(max_workers=mp.cpu_count())


async def insert_new_data(games_list, moves_list, months_list):
    """
    Inserts formatted game, move, and month data into the database in the correct order
    to respect foreign key constraints. Games must be inserted before moves.

    Args: lists for games, moves_list and month_list
            each list contains one dictionary for item.

    Returns: Nothing
    
    """
    game_interface = DBInterface(Game)
    move_interface = DBInterface(Move)
    month_interface = DBInterface(Month)

    # Step 1: Insert games first. This is crucial for foreign key integrity with moves.
    if games_list:
        await game_interface.create_all(games_list)
        print(f"Successfully inserted {len(games_list)} games.")
    else:
        print("No new games to insert.")

    # Step 2: Insert moves after games are confirmed to be in the database.
    if moves_list:
        await move_interface.create_all(moves_list)
        print(f"Successfully inserted {len(moves_list)} moves.")
    else:
        print("No new moves to insert.")

    # Step 3: Insert months. This can be done after games and moves, or even concurrently

    if months_list:
        await month_interface.create_all(months_list)
        print(f"Successfully inserted {len(months_list)} months.")
    else:
        print("No new months to insert.")

    total_inserted_items = len(games_list) + len(moves_list) + len(months_list)
    if total_inserted_items > 0:
        print(f"Overall database insertion completed for {len(games_list)} games, {len(moves_list)} moves, and {len(months_list)} months.")
    else:
        print("No data was inserted into the database.")

def get_pgn_item(game_pgn: str, item: str) -> str:
    """Extracts an item from a PGN string."""
    try:
        if item == "Termination":
            return (
                game_pgn.split(f"{item}")[1]
                .split("\n")[0]
                .replace('"', "")
                .replace("]", "")
                .lower()
            )
        return (
            game_pgn.split(f"{item}")[1]
            .split("\n")[0]
            .replace('"', "")
            .replace("]", "")
            .replace(" ", "")
            .lower()
        )
    except IndexError:
        # Handle cases where the PGN item is missing (e.g., [StartTime ""])
        # print(f"Warning: PGN item '{item}' not found or in unexpected format.")
        if item in ['StartTime', 'EndTime']:
            return "00:00:00" # Default time if missing
        if item in ['Date', 'EndDate']:
            return "0.0.0" # Default date if missing
        # Re-raise for other items
        raise

def get_start_and_end_date(game, game_for_db):
    """Extracts and calculates game start/end dates and time elapsed."""
    try:
        game_date = get_pgn_item(game['pgn'], item='Date').split('.')
        game_for_db['year'] = int(game_date[0])
        game_for_db['month'] = int(game_date[1])
        game_for_db['day'] = int(game_date[2])
    except Exception as e:
        print(f"Warning: Could not parse game date for game {game.get('url', 'N/A')}. Setting year to 0. Error: {e}")
        game_for_db['year'] = 0
        return game_for_db # Return early if date is invalid

    try:
        game_start_time_str = get_pgn_item(game['pgn'], item='StartTime').split(':')
        game_for_db['hour'] = int(game_start_time_str[0])
        game_for_db['minute'] = int(game_start_time_str[1])
        game_for_db['second'] = int(game_start_time_str[2])
    except Exception as e:
        print(f"Warning: Could not parse game start time for game {game.get('url', 'N/A')}. Setting to 0. Error: {e}")
        game_for_db['hour'] = 0
        game_for_db['minute'] = 0
        game_for_db['second'] = 0

    # Only create game_start if date parsing was successful
    if game_for_db['year'] != 0:
        game_start = datetime(year = game_for_db['year'],
                              month = game_for_db['month'],
                              day = game_for_db['day'],
                              hour = game_for_db['hour'],
                              minute = game_for_db['minute'],
                              second = game_for_db['second'])
    else:
        game_start = None # Cannot calculate time_elapsed

    try:
        game_end_date_str = get_pgn_item(game['pgn'], item='EndDate').split('.')
        game_for_db['end_year'] = int(game_end_date_str[0])
        game_for_db['end_month'] = int(game_end_date_str[1])
        game_for_db['end_day'] = int(game_end_date_str[2])
        game_end_time_str = get_pgn_item(game['pgn'], item='EndTime').split(':')
        game_for_db['end_hour'] = int(game_end_time_str[0])
        game_for_db['end_minute'] = int(game_end_time_str[1])
        game_for_db['end_second'] = int(game_end_time_str[2])

        game_end = datetime(year= game_for_db['end_year'],
                            month = game_for_db['end_month'],
                            day = game_for_db['end_day'],
                            hour = game_for_db['end_hour'],
                            minute = game_for_db['end_minute'],
                            second =game_for_db['end_second'])
        
        if game_start:
            game_for_db['time_elapsed'] = (game_end - game_start).total_seconds()
        else:
            game_for_db['time_elapsed'] = 0
            
    except Exception as e:
        print(f"Warning: Could not parse game end date/time or calculate time_elapsed for game {game.get('url', 'N/A')}. Setting to 0. Error: {e}")
        game_for_db['end_year'] = 0
        game_for_db['end_month'] = 0
        game_for_db['end_day'] = 0
        game_for_db['end_hour'] = 0
        game_for_db['end_minute'] = 0
        game_for_db['end_second'] = 0
        game_for_db['time_elapsed'] = 0

    return game_for_db

def translate_result_to_float(str_result):
    """Converts string results to float representation."""
    if str_result in WINING_RESULT:
        return 1.0
    if str_result in DRAW_RESULTS:
        return 0.5 
    if str_result in LOSE_RESULTS:
        return 0.0 
    else:
        print('""""""UNKNOWN Natural Language Result"""""""""""""')
        print(str_result)
        return None

def get_black_and_white_data(game, game_for_db):
    """Extracts white and black player data and results."""
    game_for_db['black'] = game['black']['username'].lower()
    game_for_db['black_elo'] = int(game['black']['rating'])
    game_for_db['black_str_result'] = game['black']['result'].lower()
    game_for_db['black_result'] = translate_result_to_float(game_for_db['black_str_result'])

    game_for_db['white'] = game['white']['username'].lower()
    game_for_db['white_elo'] = int(game['white']['rating'])
    game_for_db['white_str_result'] = game['white']['result'].lower()
    game_for_db['white_result'] = translate_result_to_float(game_for_db['white_str_result'])
    return game_for_db

def get_time_bonus(game):
    """Extracts time bonus from time_control string."""
    time_control = game['time_control']
    if "+" in time_control:
        return int(time_control.split("+")[-1])
    return 0

def get_n_moves(raw_moves):
    """Calculates the number of moves from a raw PGN moves string."""
    if not raw_moves.strip():
        return 0
    numeric_moves = [int(x.replace(".", "")) for x in raw_moves.split() if x.replace(".", "").isnumeric()]
    return max(numeric_moves) if numeric_moves else 0

# --- EFFICIENT VERSION ---
def _parse_time_to_seconds(time_str: str) -> float:
    """Converts 'H:M:S.f', 'M:S.f', or 'M:S' to seconds."""
    if time_str == "--":
        return 0.0
    try:
        parts = time_str.split(':')
        seconds = 0.0
        if len(parts) == 3: # H:M:S.f
            seconds = int(parts[0]) * 3600 + int(parts[1]) * 60 + float(parts[2])
        elif len(parts) == 2: # M:S.f
            seconds = int(parts[0]) * 60 + float(parts[1])
        return round(seconds, 3)
    except Exception:
        return 0.0

def _calculate_reaction_times(time_series: List[float], time_bonus: int) -> List[float]:
    """
    Re-implements pandas `diff(periods=-1).abs() + time_bonus` in pure Python.
    Calculates time_left[i] - time_left[i+1]
    """
    reaction_times = []
    for i in range(len(time_series)):
        if i < len(time_series) - 1:
            # This is the time spent on the *next* move, which is what the diff did.
            # But the logic is flawed. The reaction time for move[i] should be time_left[i-1] - time_left[i].
            # However, to preserve your original logic:
            reaction = abs(time_series[i] - time_series[i+1])
            reaction_times.append(round(reaction + time_bonus, 3))
        else:
            # Last move has no next move to diff against
            reaction_times.append(round(time_bonus, 3)) # Or 0.0, based on desired logic
    return reaction_times

def create_moves_table(
        game_url:str,
        times: list,
        clean_moves: list,
        n_moves: int,
        time_bonus: int) -> dict[str, Any]: 
    """
    Formats raw move data into a dictionary suitable for MoveCreateData.
    Calculates reaction times from the available data.
    """
    
    link = int(game_url.split('/')[-1])

    if len(clean_moves) % 2 != 0:
        clean_moves.append("--")
    if len(times) % 2 != 0:
        times.append("--")

    white_moves = []
    black_moves = []
    white_times_sec = []
    black_times_sec = []

    # 1. Convert times to seconds and split moves
    for i in range(0, len(clean_moves), 2):
        white_moves.append(str(clean_moves[i]))
        black_moves.append(str(clean_moves[i+1]))
        
        # White time
        white_times_sec.append(_parse_time_to_seconds(times[i]))
        # Black time
        if i + 1 < len(times):
            black_times_sec.append(_parse_time_to_seconds(times[i+1]))
        else:
            black_times_sec.append(0.0)

    # 2. Calculate reaction times
    # time_left[i] - time_left[i+1]
    # This is the time elapsed *during* the (i+1)th move.
    # This means the reaction time for move 2 is stored at index 1.
    
    def diff_minus_1(series):
        if not series:
            return []
        return [abs(series[i] - series[i+1]) + time_bonus if i < len(series) - 1 else time_bonus for i in range(len(series))]

    white_reaction_times = [round(x, 3) for x in diff_minus_1(white_times_sec)]
    black_reaction_times = [round(x, 3) for x in diff_minus_1(black_times_sec)]

    result = {
        "link": link,
        "white_moves": white_moves,
        "white_reaction_times": white_reaction_times,
        "white_time_left": white_times_sec,
        "black_moves": black_moves,
        "black_reaction_times": black_reaction_times,
        "black_time_left": black_times_sec
    }
    return result
# --- END EFFICIENT VERSION ---

def get_moves_data(game: dict) -> tuple[int, dict]:
    """Extracts and formats the moves of a game."""
    time_bonus = get_time_bonus(game)

    raw_moves = (
        game['pgn'].split("\n\n")[1]
        .replace("1/2-1/2", "")
        .replace("1-0", "")
        .replace("0-1", "")
    )
    n_moves = get_n_moves(raw_moves)

    times = [x.replace("]", "").replace("}", "") for x in raw_moves.split() if ":" in x]
    just_moves = re.sub(r"{[^}]*}*", "", raw_moves)
    clean_moves = [x for x in just_moves.split() if x and "." not in x]
    
    if len(clean_moves) % 2 != 0:
        clean_moves.append("--")

    if len(times) % 2 != 0:
        times.append("--")

    moves_data = create_moves_table(game['url'],
                                    times,
                                    clean_moves,
                                    n_moves,
                                    time_bonus)
    return n_moves, moves_data

def create_game_dict(game_raw_data: dict) -> Union[Dict[str, Any], str, bool]:
    """Converts raw game data into a dictionary for the Game model."""
    try:
        len(game_raw_data['pgn'])
    except KeyError:
        return "NO PGN"

    game_for_db = dict()
    game_for_db['fens_done'] = False
    game_for_db['link'] = int(game_raw_data['url'].split('/')[-1])
    game_for_db['time_control'] = game_raw_data['time_control']
    game_for_db = get_start_and_end_date(game_raw_data, game_for_db)

    if game_for_db['year'] == 0:
        print(f"Skipping game {game_raw_data.get('url', 'N/A')} due to date parsing error.")
        return False

    game_for_db = get_black_and_white_data(game_raw_data, game_for_db)

    if game_for_db['white_result'] is None or game_for_db['black_result'] is None:
        print(f"Skipping game {game_raw_data.get('url', 'N/A')} due to unrecognised result string.")
        return False

    try:
        n_moves, moves_data = get_moves_data(game_raw_data)
    except Exception as e:
        #print(f"Error getting moves data for game {game_raw_data.get('url', 'N/A')}: {e}")
        return False

    game_for_db['n_moves'] = n_moves
    game_for_db['moves_data'] = moves_data
    try:
        game_for_db['eco'] = game_raw_data['eco']
    except Exception: # Broad exception if ECO tag is missing
        game_for_db['eco'] = 'no_eco'
    return game_for_db

def format_one_game_moves(moves: dict) -> List[Dict[str, Any]]:
    """Formats individual moves data for the Move model."""
    to_insert_moves = []
    try:
        # Ensure 'white_moves', 'black_moves', etc. are present and are lists
        if not all(k in moves and isinstance(moves[k], list) for k in ['white_moves', 'white_reaction_times', 'white_time_left', 'black_moves', 'black_reaction_times', 'black_time_left']):
            print(f"Warning: Missing or invalid moves data structure for game link {moves.get('link', 'N/A')}")
            return []
    except KeyError:
        return []

    # Ensure all lists are of comparable length, or handle index errors gracefully
    max_len = len(moves['white_moves'])

    for ind in range(max_len):
        moves_dict = {}
        moves_dict['n_move'] = ind + 1
        moves_dict['link'] = moves['link']

        # White's move data
        moves_dict['white_move'] = str(moves['white_moves'][ind])
        moves_dict['white_reaction_time'] = round(moves['white_reaction_times'][ind], 3) if ind < len(moves['white_reaction_times']) else 0.0
        moves_dict['white_time_left'] = round(moves['white_time_left'][ind], 3) if ind < len(moves['white_time_left']) else 0.0

        # Black's move data (handle potential IndexError if black has fewer moves)
        try:
            moves_dict['black_move'] = str(moves['black_moves'][ind])
            moves_dict['black_reaction_time'] = round(moves['black_reaction_times'][ind], 3) if ind < len(moves['black_reaction_times']) else 0.0
            moves_dict['black_time_left'] = round(moves['black_time_left'][ind], 3) if ind < len(moves['black_time_left']) else 0.0
        except IndexError:
            moves_dict['black_move'] = '--'
            moves_dict['black_reaction_time'] = 0.0
            moves_dict['black_time_left'] = 0.0

        # Validate and convert to Pydantic model, then dump to dict
        try:
            to_insert_moves.append(MoveCreateData(**moves_dict).model_dump())
        except Exception as e:
            print(f"Error creating MoveCreateData for move {ind+1} of game {moves.get('link', 'N/A')}: {e}")
            # Decide whether to skip this move or the whole game, for now just skip this move
            continue
    return to_insert_moves


# --- MAIN FORMAT AND INSERT FUNCTION ---

async def format_games(games, player_name) -> Union[List[Dict[str, Any]], str]:
    """
    Formats and inserts downloaded games into the database efficiently.
    --- OPTIMIZED VERSION ---
    """
    player_interface = DBInterface(Player)
    start_overall = time.time()
    

    # Step 1: Filter out games already in DB (I/O-bound)
    start_filter = time.time()
    games_to_process = await get_just_new_games(games)
    if not games_to_process: # get_just_new_games returns False if no new games or error
        print(f"No new games to process for {player_name}. All games already at DB or input was empty.")
        return "All games already at DB"
    
    num_games_to_process = sum(len(m_games) for y_games in games_to_process.values() for m_games in y_games.values())
    print(f"Filtered {num_games_to_process} new games in: {time.time() - start_filter:.2f} seconds")

    # Step 2: Collect all unique players from the new games
    start_get_unique_players = time.time()
    unique_player_names = set()
    for year_games in games_to_process.values():
        for month_games in year_games.values():
            for game_raw_data in month_games:
                if 'white' in game_raw_data and 'username' in game_raw_data['white']:
                    unique_player_names.add(game_raw_data['white']['username'].lower())
                if 'black' in game_raw_data and 'username' in game_raw_data['black']:
                    unique_player_names.add(game_raw_data['black']['username'].lower())     
    
    # Step 3: Find players *not* in the DB
    players_not_in_db = await get_only_players_not_in_db(unique_player_names)
    print(f"Found {len(players_not_in_db)} new players to insert in: {time.time() - start_get_unique_players:.2f}s")
    
    # Step 4: Insert *only* the new players (as 'shell' players, without full profiles)
    start_inserting_players = time.time()
    if players_not_in_db:
        # Create minimal player dicts for insertion
        player_insertion_data = [
            {"player_name": p_name, "joined": 0} for p_name in players_not_in_db
        ]
        await player_interface.create_all(player_insertion_data)
        print(f"Inserted {len(players_not_in_db)} new 'shell' players in: {time.time() - start_inserting_players:.2f} seconds")

    # Step 5: Format games (CPU-bound, run in parallel)
    start_format = time.time()
    games_futures = []
    
    # Flatten the games_to_process to a single list of game_raw_data
    all_raw_games_to_format = [
        game_raw_data
        for year_games in games_to_process.values()
        for month_games in year_games.values()
        for game_raw_data in month_games
    ]

    # --- OPTIMIZATION: Schedule each game format as a separate thread task ---
    for game_raw_data in all_raw_games_to_format:
        game_future = asyncio.to_thread(create_game_dict, game_raw_data)
        games_futures.append(game_future)

    # Await all game formatting futures concurrently
    formatted_games_results = await asyncio.gather(*games_futures)
    
    # Filter out failed formats (None, False, "NO PGN")
    valid_formatted_games = [
        g for g in formatted_games_results if g and g != "NO PGN"
    ]
    
    print(f'Formatted {len(valid_formatted_games)} games (out of {len(all_raw_games_to_format)}) in {time.time()-start_format:.2f}s')
    
    # Return the list of valid, formatted game dictionaries
    return valid_formatted_games

async def insert_games_months_moves_and_players(formatted_games_results: List[Dict[str, Any]], player_name: str):
    """
    Takes the list of formatted game dictionaries and inserts games, moves, and months.
    --- OPTIMIZED VERSION ---
    """
    
    games_list_for_db = []
    moves_futures = [] # Store futures for move formatting
    months_processed_count = {} # { (year, month): count }
    
    start_moves_format = time.time()

    for game_dict_result in formatted_games_results:
        # We already filtered in the calling function, but double-check
        if not game_dict_result: 
            continue
            
        try:
            moves_data = game_dict_result.pop('moves_data', None)
        except Exception: 
            continue
        
        if moves_data:
            # --- OPTIMIZATION: Schedule each move format as a separate thread task ---
            moves_formatted_future = asyncio.to_thread(format_one_game_moves, moves_data)
            moves_futures.append(moves_formatted_future)

        # Prepare the game data for insertion
        try:
            games_list_for_db.append(GameCreateData(**game_dict_result).model_dump())
            
            # Update month counts
            game_year = game_dict_result.get('year')
            game_month = game_dict_result.get('month')
            if game_year and game_month:
                key = (int(game_year), int(game_month))
                months_processed_count[key] = months_processed_count.get(key, 0) + 1

        except Exception as e:
            print(f"Error creating GameCreateData for formatted game {game_dict_result.get('link', 'N/A')}: {e}. Skipping game.")
            continue
            
    # --- Await all move formatting tasks ---
    moves_list_results = await asyncio.gather(*moves_futures)
    
    # Flatten the list of lists of moves
    moves_list_for_db = [
        move for moves_list in moves_list_results for move in moves_list
    ]
    
    print(f'Formatted {len(moves_list_for_db)} moves in {time.time()-start_moves_format:.2f}s')

    print(f'{len(games_list_for_db)} Games ready to insert')
    print(f'{len(moves_list_for_db)} Moves ready to insert')

    # Create months_list_for_db from the collected counts
    months_list_for_db = []
    for (year, month), n_games in months_processed_count.items():
        month_data = {
            "player_name": player_name,
            "year": year,
            "month": month,
            "n_games": n_games
        }
        months_list_for_db.append(MonthCreateData(**month_data).model_dump())

    # Step 4: Insert data into DB (I/O-bound, run concurrently)
    if not games_list_for_db and not moves_list_for_db and not months_list_for_db:
        print("No data to insert after formatting. Skipping database insertion.")
        return f"No new data to insert for {player_name}."

    start_insert = time.time()
    await insert_new_data(games_list_for_db, moves_list_for_db, months_list_for_db)
    print(f'Inserted games, moves, and months for {len(games_list_for_db)} games in: {time.time()-start_insert:.2f} seconds')

    print(f"Total time for insert_games_months_moves_and_players: {(time.time()-start_moves_format):.2f} seconds")

    return f"Successfully processed and inserted {len(games_list_for_db)} games for {player_name}."


async def get_just_new_games(games: Dict[str, Dict[str, List[Dict[str, Any]]]]) -> Union[Dict[str, Dict[str, List[Dict[str, Any]]]], bool]:
    """
    Asynchronously checks the available games and returns only those not already in the DB.
    
    --- OPTIMIZED VERSION ---
    This version iterates once to build a lookup map, then builds the new
    dictionary from the filtered list of new links, avoiding a second O(N) loop.
    """
    
    # Step 1: Create a lookup map of link -> (game_object, year, month)
    # This is O(N) but we only do it once.
    game_map: Dict[int, Tuple[Dict[str, Any], str, str]] = {}
    for year, month_data in games.items():
        for month, games_in_month in month_data.items():
            for game in games_in_month:
                try:
                    if game['url']:
                        game_link = int(game['url'].split('/')[-1])
                        game_map[game_link] = (game, year, month)
                    else:
                        print(f"Warning: Game found with no URL.")
                except Exception as e:
                    print(f"Error processing game for link extraction: {e}, game data: {game}")
                    continue

    links_to_check = set(game_map.keys())
    if not links_to_check:
        print("No valid game links found to check against the database.")
        return False

    # Step 2: Asynchronously get games already in the database
    in_db_game_links = await get_games_already_in_db(tuple(links_to_check))

    to_insert_game_links = links_to_check - in_db_game_links

    if not to_insert_game_links:
        print("All available games are already in the database.")
        return False

    # Step 3: Reconstruct the nested dictionary from the filtered links
    # This loop is O(M) where M is the number of NEW games (M <= N).
    new_games_structured: Dict[str, Dict[str, List[Dict[str, Any]]]] = {}
    for link in to_insert_game_links:
        game, year, month = game_map[link]
        
        if year not in new_games_structured:
            new_games_structured[year] = {}
        if month not in new_games_structured[year]:
            new_games_structured[year][month] = []
            
        new_games_structured[year][month].append(game)

    total_new_games_count = len(to_insert_game_links)
    if total_new_games_count == 0:
        # This check is technically redundant now but good for safety
        print("After filtering, no new games remain.")
        return False

    return new_games_structured



### inner_api/operations/fens.py 
# chessism_api/operations/fens.py

import asyncio
import chess
import time
from typing import List, Dict, Any, Tuple
from sqlalchemy import select, update, func
from sqlalchemy.ext.asyncio import AsyncSession
from datetime import datetime
import os

from chessism_api.database.engine import AsyncDBSession
from chessism_api.database.models import Game, Move, Fen
from chessism_api.database.db_interface import DBInterface

# ---
# 1. YOUR HELPER FUNCTIONS
# ---

def process_single_game_sync(game_data: Tuple[int, List[Dict[str, Any]]]) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    Takes a game's moves, reconstructs the game, and generates a simplified FEN 
    for every half-move.
    
    This function is CPU-bound and is intended to run in a thread executor.
    
    Args:
        game_data: A tuple (game_link: int, moves: List[Dict])
        
    Returns:
        A tuple: (List[Successful FENs], List[Failure Details])
    """
    link, one_game_moves = game_data
    fens_to_insert = []
    failures = [] # <-- NEW: To store failure info
    board = chess.Board()
    
    # Sort moves by move number (n_move)
    try:
        data = sorted(one_game_moves, key=lambda x: x['n_move'])
    except KeyError:
        # print(f"KeyError sorting moves for game {link}. Skipping game.") # <-- SILENCED
        failures.append({'link': link, 'move_num': -1, 'san': 'N/A', 'error': 'KeyError on move sort'})
        return ([], failures) # Return failure

    try:
        for ind, move_data in enumerate(data):
            # Check for data integrity: moves should be sequential
            expected_move_num = ind + 1
            current_move_num = move_data.get('n_move')

            if not expected_move_num == current_move_num:
                # print(f"Move sequence mismatch in game {link}. Skipping game.") # <-- SILENCED
                failures.append({'link': link, 'move_num': current_move_num, 'san': 'N/A', 'error': 'Move sequence mismatch'})
                break # Stop processing this game

            white_move_san = move_data.get('white_move')
            black_move_san = move_data.get('black_move')

            # --- Process White's Move ---
            if white_move_san and white_move_san != "--":
                try:
                    move_obj_white = board.parse_san(white_move_san)
                    board.push(move_obj_white)
                    current_fen_white = board.fen()
                    # Use float 'ind' for half-move counter
                    to_insert_white = simplify_fen(current_fen_white, float(ind) + 0.0, link)
                    fens_to_insert.append(to_insert_white)
                except ValueError as e:
                    # --- MODIFIED: Log error details instead of printing ---
                    failures.append({
                        'link': link, 
                        'move_num': current_move_num, 
                        'color': 'white', 
                        'san': white_move_san, 
                        'error': str(e)
                    })
                    break # Stop processing this game

            # --- Process Black's Move ---
            if black_move_san and black_move_san != "--":
                try:
                    move_obj_black = board.parse_san(black_move_san)
                    board.push(move_obj_black)
                    current_fen_black = board.fen()
                    # Use float 'ind + 0.5' for half-move counter
                    to_insert_black = simplify_fen(current_fen_black, float(ind) + 0.5, link)
                    fens_to_insert.append(to_insert_black)
                except ValueError as e:
                    # --- MODIFIED: Log error details instead of printing ---
                    failures.append({
                        'link': link, 
                        'move_num': current_move_num, 
                        'color': 'black', 
                        'san': black_move_san, 
                        'error': str(e)
                    })
                    break # Stop processing this game

    except Exception as e:
        # print(f"Unexpected error processing game {link}: {e}") # <-- SILENCED
        failures.append({'link': link, 'move_num': -1, 'san': 'N/A', 'error': f"Unexpected processing error: {e}"})

    return (fens_to_insert, failures) # <-- NEW: Return tuple


def simplify_fen(raw_fen: str, n_move: float, link:int) -> Dict[str, Any]:
    """
    Simplifies the FEN to the first 4 components (board, side, castling, en passant target).
    Adds metadata for insertion into the Fen table.
    """
    parts = raw_fen.split(' ')
    
    simplified_fen = ' '.join(parts[:4])
    
    # moves_counter stores the halfmove clock and fullmove number for tracking purposes
    moves_counter = f"#{parts[4]}#{parts[5]}_" 
    
    return {'link':link, # This will be filtered out by DBInterface
            'fen':simplified_fen,
            'n_games':1, # Initial count is 1 for the game being processed
            'moves_counter': moves_counter,
            'n_move' : n_move, # This will be filtered out by DBInterface
            'next_moves' : None,
            'score' : None}

# ---
# 2. NEW DATABASE HELPER FUNCTIONS
# ---

async def _get_games_needing_fens(session: AsyncSession, batch_size: int) -> List[int]:
    """
    Queries the database for game links where FENs have not been generated yet.
    """
    # print(f"Fetching new batch of {batch_size} games...", flush=True) # <-- SILENCED
    stmt = (
        select(Game.link)
        .where(Game.fens_done == False)
        .limit(batch_size)
    )
    result = await session.execute(stmt)
    game_links = result.scalars().all()
    # print(f"Found {len(game_links)} games to process.", flush=True) # <-- SILENCED
    return game_links

async def _get_moves_for_games(session: AsyncSession, game_links: List[int]) -> Dict[int, List[Dict[str, Any]]]:
    """
    Fetches all moves associated with a list of game links.
    """
    if not game_links:
        return {}
        
    # print(f"Fetching moves for {len(game_links)} games...", flush=True) # <-- SILENCED
    stmt = (
        select(Move.link, Move.n_move, Move.white_move, Move.black_move)
        .where(Move.link.in_(game_links))
        .order_by(Move.link, Move.n_move)
    )
    result = await session.execute(stmt)
    
    # Group moves by game link
    moves_by_link: Dict[int, List[Dict[str, Any]]] = {}
    for row in result.mappings(): # .mappings() returns dictionaries
        link = row['link']
        if link not in moves_by_link:
            moves_by_link[link] = []
        moves_by_link[link].append(row)
        
    # print(f"Found moves for {len(moves_by_link)} games.", flush=True) # <-- SILENCED
    return moves_by_link

async def _get_remaining_fens_count(session: AsyncSession) -> int:
    """
    Counts how many games still have fens_done = False.
    Uses the provided session to read the current transaction state.
    """
    stmt = select(func.count(Game.link)).where(Game.fens_done == False)
    result = await session.execute(stmt)
    count = result.scalar()
    return count or 0

async def _get_remaining_fens_count_committed() -> int:
    """
    Counts how many games still have fens_done = False using a NEW session.
    This reads the last COMMITTED state of the database.
    """
    async with AsyncDBSession() as session:
        stmt = select(func.count(Game.link)).where(Game.fens_done == False)
        result = await session.execute(stmt)
        count = result.scalar()
        return count or 0

# ---
# 3. ORCHESTRATOR / BACKGROUND JOB (MODIFIED)
# ---

async def run_fen_generation_job(total_games_to_process: int = 1000000, batch_size: int = 1000):
    """
    The main background task.
    - Loops until 'total_games_to_process' is met or no games are left.
    - Pre-aggregates FENs in Python to avoid CardinalityViolationError.
    - Logs total job time and failures at the end.
    - MODIFIED: Each batch is now its own atomic transaction.
    """
    
    # --- MODIFIED: Added flush=True ---
    print(f"--- [START] FEN Generation Job ---", flush=True)
    print(f"Targeting {total_games_to_process} games, in batches of {batch_size}.", flush=True)
    
    fen_interface = DBInterface(Fen)
    total_processed_so_far = 0
    total_games_failed = 0 
    total_job_failures_list = [] 
    
    job_start_time = time.time()
    
    # --- MODIFIED: Session is now created *inside* the loop ---
    while total_processed_so_far < total_games_to_process:
        
        # --- MODIFIED: Added flush=True ---
        print(f"[FEN JOB] Processing batch { (total_processed_so_far // batch_size) + 1 }...", flush=True)
        batch_start_time = time.time()
        
        # Each loop is a new session and a new atomic transaction
        async with AsyncDBSession() as session:
            try:
                # Calculate how many games to fetch in this batch
                games_left_to_reach_target = total_games_to_process - total_processed_so_far
                current_batch_size = min(batch_size, games_left_to_reach_target)
                
                # 1. Fetch Games (uses this batch's session)
                game_links = await _get_games_needing_fens(session, current_batch_size)
                if not game_links:
                    print("[FEN JOB] No more games found to process. Stopping job.", flush=True)
                    break # No more games left
                
                # --- MODIFIED: Added flush=True ---
                print(f"[FEN JOB] Fetched {len(game_links)} games for this batch.", flush=True)

                # 2. Fetch Moves (uses this batch's session)
                moves_by_link = await _get_moves_for_games(session, game_links)
                
                # 3. Process in Parallel (CPU-bound)
                tasks_data = [
                    (link, moves) for link, moves in moves_by_link.items() if moves
                ]
                tasks = [
                    asyncio.to_thread(process_single_game_sync, game_data)
                    for game_data in tasks_data
                ]
                results_tuples = await asyncio.gather(*tasks) 
                
                batch_fens_to_aggregate = []
                
                for fens_list, failures_list in results_tuples:
                    if failures_list:
                        total_games_failed += 1
                        total_job_failures_list.extend(failures_list)
                    else:
                        batch_fens_to_aggregate.extend(fens_list)
                
                # 4. Pre-aggregate FENs
                aggregated_fens: Dict[str, Dict[str, Any]] = {}
                for fen_data in batch_fens_to_aggregate: 
                    fen_str = fen_data['fen']
                    if fen_str not in aggregated_fens:
                        aggregated_fens[fen_str] = {
                            'fen': fen_str,
                            'n_games': 1,
                            'moves_counter': fen_data['moves_counter'],
                            'next_moves': None,
                            'score': None
                        }
                    else:
                        aggregated_fens[fen_str]['n_games'] += 1
                        aggregated_fens[fen_str]['moves_counter'] += fen_data['moves_counter']
                all_fens_to_insert = list(aggregated_fens.values())

                # --- MODIFIED: Added flush=True ---
                print(f"[FEN JOB] Aggregated {len(all_fens_to_insert)} unique FENs from {len(tasks_data)} successful games.", flush=True)

                # 5. Bulk-Upsert FENs (uses this batch's session)
                if all_fens_to_insert:
                    # --- MODIFIED: Pass the session to create_all ---
                    await fen_interface.create_all_with_session(session, all_fens_to_insert)

                # 6. Mark Games as Done (uses this batch's session)
                await _mark_games_as_done_in_session(session, game_links)
                
                # 7. Commit Transaction
                # If all steps above succeeded, commit the transaction
                await session.commit()
                
                total_processed_so_far += len(game_links)
                
                # --- MODIFIED: Added flush=True ---
                batch_end_time = time.time()
                print(f"[FEN JOB] Batch complete. Total games: {total_processed_so_far}. Batch Time: {(batch_end_time - batch_start_time):.2f}s", flush=True)


            except Exception as e:
                # If anything in this batch failed, roll back the transaction
                print(f"CRITICAL: Error during batch processing: {e}. Rolling back batch.", flush=True)
                if hasattr(e, 'orig'):
                    print(f"DBAPI Error: {e.orig}", flush=True)
                await session.rollback()
                break # Stop the job
            
        # --- END OF WHILE LOOP ---
    
    # 7. Write failures (if any)
    if total_job_failures_list:
        log_file_path = "logs/illegall_fen.txt" 
        print(f"Encountered {total_games_failed} game failures (illegal SAN). Writing details to '{log_file_path}'...", flush=True)
        try:
            os.makedirs(os.path.dirname(log_file_path), exist_ok=True)
            with open(log_file_path, "a", encoding="utf-8") as f:
                f.write(f"\n--- FEN Generation Job Run: {datetime.now().isoformat()} ---\n")
                for fail in total_job_failures_list:
                    f.write(f"{fail}\n")
                f.write(f"--- End of Job Run ---\n")
        except Exception as e:
            print(f"CRITICAL: Failed to write to '{log_file_path}': {e}", flush=True)
    
    
    # 8. Log final summary (using a new session for a committed read)
    total_job_time = time.time() - job_start_time
    remaining_games_count = await _get_remaining_fens_count_committed()
    
    print(f"--- [END] FEN Generation Job ---", flush=True)
    print(f"--- Total job time: {total_job_time:.2f} seconds ---", flush=True)
    print(f"--- Total games processed in this run: {total_processed_so_far} ---", flush=True)
    print(f"--- Games failed (illegal SAN): {total_games_failed} ---", flush=True)
    print(f"--- Games still needing FENs in DB: {remaining_games_count} ---", flush=True)


async def _mark_games_as_done_in_session(session: AsyncSession, game_links: List[int]):
    """
    (Helper for run_fen_generation_job)
    Bulk updates the Game table to set fens_done=True using the provided session.
    """
    if not game_links:
        return
    
    stmt = (
        update(Game)
        .where(Game.link.in_(game_links))
        .values(fens_done=True)
    )
    await session.execute(stmt)
    # The commit is now handled by the main job loop
    # print(f"Successfully marked {len(game_links)} games as fens_done=True (pending commit).", flush=True) # <-- SILENCED



### inner_api/operations/players.py 
#chessism_api/operations/players.py

import asyncio # <-- NEW
import time # <-- NEW
from fastapi.encoders import jsonable_encoder
from typing import Optional, Union, Dict, Any, Tuple, List
from sqlalchemy.exc import IntegrityError
from sqlalchemy import select, update

# --- FIXED IMPORTS ---
from chessism_api.database.db_interface import DBInterface
from chessism_api.database.models import Player, PlayerStats, to_dict # <-- NEW (PlayerStats, to_dict)
from chessism_api.database.ask_db import open_async_request
from chessism_api.operations.models import PlayerCreateData, PlayerStatsCreateData # <-- NEW
from chessism_api.operations.chess_com_api import get_profile, get_player_stats # <-- NEW
# ---

async def read_player(player_name: str) -> Optional[Dict[str, Any]]:
    """
    Reads a player's profile from the database by player_name.
    """
    player_db_interface = DBInterface(Player)
    player_name_lower = player_name.lower()
    
    # --- FIX: .read() returns a list ---
    player_list = await player_db_interface.read(player_name=player_name_lower)

    if player_list:
        return player_list[0] # Return the first (and only) player dict
    else:
        print(f"Player {player_name_lower} not found in DB.")
        return None

async def insert_player(data: dict) -> Optional[PlayerCreateData]:
    """
    Inserts a new player into the database or updates an existing one.
    Fetches the full profile from Chess.com.
    """
    player_name_lower = data['player_name'].lower()
    player_interface = DBInterface(Player)

    fetched_profile: Optional[PlayerCreateData] = await get_profile(player_name_lower)

    if fetched_profile is None:
        # This can happen if Chess.com API fails (e.g., timeout, 404)
        # We might still want to create a shell player if one doesn't exist
        # For now, we'll follow the logic of returning None if fetch fails
        print(f"Profile for {player_name_lower} came back as None from Chess.com.")
        
        # Check if a shell player exists, if not, create one.
        existing_player = await read_player(player_name_lower)
        if existing_player:
            return PlayerCreateData(**existing_player) # Return existing shell data

        # Create a shell player if fetch fails AND player doesn't exist
        print(f"Creating 'shell' player for {player_name_lower} as profile fetch failed.")
        shell_data = {"player_name": player_name_lower, "joined": 0}
        try:
            created_shell_dict = await player_interface.create(shell_data)
            return PlayerCreateData(**created_shell_dict)
        except IntegrityError:
             # Race condition: another process created it. Just read it.
             existing_player = await read_player(player_name_lower)
             return PlayerCreateData(**existing_player) if existing_player else None
        except Exception as e:
            print(f"Error creating shell player {player_name_lower}: {e}")
            return None


    # --- FIX: Convert Pydantic model to dict for DBInterface ---
    fetched_profile_dict = fetched_profile.model_dump()

    try:
        print(f"Attempting to insert new player profile for: {player_name_lower}")
        created_player_dict = await player_interface.create(fetched_profile_dict)
        print(f'NEW player {player_name_lower} inserted')
        # --- FIX: Convert dict back to Pydantic model for return type ---
        return PlayerCreateData(**created_player_dict)
        
    except IntegrityError: 
        # Player already exists (e.g., as a shell player). Update them.
        print(f"Player {player_name_lower} already exists. Updating.")
        try:
            # --- FIX: Use correct .update() method ---
            # We use player_name_lower as the primary key
            updated_player_dict = await player_interface.update(player_name_lower, fetched_profile_dict)
            if updated_player_dict:
                 # --- FIX: Convert dict back to Pydantic model ---
                return PlayerCreateData(**updated_player_dict)
            else:
                # This should not happen if IntegrityError was raised, but as a safeguard:
                print(f"Update failed for {player_name_lower}, player not found by update method.")
                return None
        except Exception as update_e:
            print(f"Error updating player {player_name_lower} after failed insert: {update_e}")
            return None
    except Exception as e:
        print(f"An unexpected error occurred during player creation for {player_name_lower}: {e}")
        return None


async def get_current_players_with_games_in_db() -> List[Dict[str, Any]]:
    """
    Fetches all players that have a full profile (joined != 0).
    """
    query = """
    SELECT * FROM player
    WHERE joined != 0;
    """
    players = await open_async_request(query, fetch_as_dict=True)
    return players


# --- Functions for PlayerStats ---

async def read_player_stats(player_name: str) -> Optional[Dict[str, Any]]:
    """
    Reads a player's stats from the database by player_name.
    """
    stats_db_interface = DBInterface(PlayerStats)
    player_name_lower = player_name.lower()
    
    stats_list = await stats_db_interface.read(player_name=player_name_lower)
    
    if stats_list:
        return stats_list[0] # Return the first (and only) stats dict
    else:
        print(f"Stats for {player_name_lower} not found in DB.")
        return None

def _parse_stats_category(raw_stats: Dict[str, Any], category: str) -> Dict[str, Any]:
    """Helper to safely parse one stats category (e.g., 'chess_rapid')."""
    data = raw_stats.get(category, {})
    
    # Handle cases where data is None
    if data is None:
        data = {}

    last = data.get('last', {}) or {}
    best = data.get('best', {}) or {}
    record = data.get('record', {}) or {}
    
    return {
        f"{category}_last_rating": last.get('rating'),
        f"{category}_best_rating": best.get('rating'),
        f"{category}_games": record.get('game_count'), # Custom mapping
        f"{category}_wins": record.get('win'),
        f"{category}_losses": record.get('loss'),
        f"{category}_draws": record.get('draw'),
        # --- UPDATED: Check for percentile OR percentage_rank ---
        f"{category}_last_percentile": last.get('percentile') or last.get('percentage_rank')
    }

async def create_and_store_player_stats(player_name: str) -> Optional[PlayerStatsCreateData]:
    """
    Fetches fresh stats from Chess.com and performs an "upsert" (insert or update)
    into the database.
    """
    player_name_lower = player_name.lower()
    
    # 1. Ensure the player exists in the Player table first.
    # We must have a Player record to satisfy the foreign key constraint.
    player = await read_player(player_name_lower)
    if not player:
        # If player doesn't exist, create a shell record (or full profile)
        print(f"Player {player_name_lower} not found. Creating shell record before adding stats.")
        player_profile = await insert_player({"player_name": player_name_lower})
        if not player_profile:
            print(f"Failed to create player {player_name_lower}. Cannot add stats.")
            return None

    # 2. Fetch the raw stats from Chess.com API
    raw_stats = await get_player_stats(player_name_lower)
    if not raw_stats:
        print(f"Failed to fetch stats from Chess.com for {player_name_lower}.")
        return None
        
    # --- Debug Print ---
    print(f"--- RAW STATS FOR {player_name_lower} ---")
    import pprint
    pprint.pprint(raw_stats)
    print("---------------------------------")
    # ---
    
    # 3. Parse the raw stats into the Pydantic model format
    parsed_data = {"player_name": player_name_lower}
    
    parsed_data.update(_parse_stats_category(raw_stats, 'chess_rapid'))
    parsed_data.update(_parse_stats_category(raw_stats, 'chess_blitz'))
    parsed_data.update(_parse_stats_category(raw_stats, 'chess_bullet'))

    parsed_data['fide'] = raw_stats.get('fide')
    parsed_data['puzzle_rush_best_score'] = raw_stats.get('puzzle_rush', {}).get('best', {}).get('score')
    parsed_data['tactics_highest_rating'] = raw_stats.get('tactics', {}).get('highest', {}).get('rating')
    parsed_data['tactics_lowest_rating'] = raw_stats.get('tactics', {}).get('lowest', {}).get('rating')

    try:
        # Validate the parsed data
        stats_data = PlayerStatsCreateData(**parsed_data)
    except Exception as e:
        print(f"Error validating parsed stats for {player_name_lower}: {e}")
        return None

    # 4. Perform an "UPSERT"
    stats_interface = DBInterface(PlayerStats)
    stats_data_dict = stats_data.model_dump()

    try:
        # Try to create a new record
        created_stats = await stats_interface.create(stats_data_dict)
        print(f"Successfully inserted new stats for {player_name_lower}.")
        return PlayerStatsCreateData(**created_stats)
    except IntegrityError:
        # It already exists, so update it
        print(f"Stats for {player_name_lower} already exist. Updating.")
        try:
            updated_stats = await stats_interface.update(player_name_lower, stats_data_dict)
            if updated_stats:
                return PlayerStatsCreateData(**updated_stats)
            return None
        except Exception as e:
            print(f"Error updating stats for {player_name_lower}: {e}")
            return None
    except Exception as e:
        print(f"An unexpected error occurred during stats upsert: {e}")
        return None

# --- NEW FUNCTION ---
async def update_stats_for_all_primary_players(api_delay: float = 1.0) -> Dict[str, List[str]]:
    """
    Fetches stats for all primary players (joined != 0).
    Includes a delay to be respectful to the Chess.com API.
    
    Returns:
        A dictionary categorizing successful and failed player updates.
    """
    print("Starting batch job: Update stats for all primary players...")
    primary_players = await get_current_players_with_games_in_db()
    
    if not primary_players:
        print("No primary players found in the database.")
        return {"success": [], "failed": []}
        
    print(f"Found {len(primary_players)} primary players to update.")
    
    success_list = []
    failed_list = []
    
    for i, player in enumerate(primary_players):
        player_name = player.get('player_name')
        if not player_name:
            continue
            
        print(f"Updating stats for: {player_name} ({i+1}/{len(primary_players)})...")
        try:
            stats = await create_and_store_player_stats(player_name)
            if stats:
                success_list.append(player_name)
            else:
                print(f"Failed to update stats for {player_name} (API returned None).")
                failed_list.append(player_name)
        except Exception as e:
            print(f"An exception occurred while updating stats for {player_name}: {repr(e)}")
            failed_list.append(player_name)
            
        # --- API Delay ---
        # Wait before the next request, even if this one failed
        await asyncio.sleep(api_delay)
        
    print("--- Batch Stats Update Complete ---")
    print(f"Successfully updated: {len(success_list)}")
    print(f"Failed to update: {len(failed_list)}")
    if failed_list:
        print(f"Failed players: {', '.join(failed_list)}")
        
    return {"success": success_list, "failed": failed_list}



### inner_api/operations/models.py 
from typing import Optional, List, Any, Dict # <-- Import Dict
from pydantic import BaseModel, Field

# --- Pydantic model for creating a Player (used in operations/players.py) ---
class PlayerCreateData(BaseModel):
    player_name: str
    name: Optional[str] = None
    url: Optional[str] = None
    title: Optional[str] = None
    avatar: Optional[str] = None
    followers: Optional[int] = None
    country: Optional[str] = None
    location: Optional[str] = None
    joined: Optional[int] = 0 # Default to 0 for 'shell' players
    status: Optional[str] = None
    is_streamer: Optional[bool] = False
    twitch_url: Optional[str] = None
    verified: Optional[bool] = False
    league: Optional[str] = None
    
    class Config:
        # Allows Pydantic to read from ORM models (e.g., Player)
        from_attributes = True 

# --- Pydantic model for creating a Game (used in operations/format_games.py) ---
class GameCreateData(BaseModel):
    link: int
    white: str
    black: str
    year: int
    month: int
    day: int
    hour: int
    minute: int
    second: int
    white_elo: int
    black_elo: int
    white_result: float
    black_result: float
    white_str_result: str
    black_str_result: str
    time_control: str
    eco: str
    time_elapsed: float # Matches the Float in models.py
    n_moves: int
    fens_done: bool

# --- Pydantic model for creating a Move (used in operations/format_games.py) ---
class MoveCreateData(BaseModel):
    link: int
    n_move: int
    white_move: str
    black_move: str
    white_reaction_time: float
    black_reaction_time: float
    white_time_left: float
    black_time_left: float

# --- Pydantic model for creating a Month (used in operations/format_games.py) ---
class MonthCreateData(BaseModel):
    player_name: str
    year: int
    month: int
    n_games: int

# --- Pydantic model for returning a Month (used in operations/months.py) ---
class MonthResult(MonthCreateData):
    id: int # Include the 'id' from the database
    
    class Config:
        from_attributes = True # Allow Pydantic to read from ORM models

# --- Pydantic models for FEN operations (used in db_interface.py) ---
class FenCreateData(BaseModel):
    fen: str
    n_games: int
    moves_counter: str
    next_moves: Optional[str] = None
    score: Optional[float] = None

class FenGameAssociateData(BaseModel):
    fen_fen: str  # The FEN string to associate
    game_link: int

# --- Pydantic model for creating AnalysisTimes (used in ask_db.py) ---
class AnalysisTimesCreateData(BaseModel):
    batch_index: int
    n_batches:int
    card: int
    model: str
    n_fens: int
    time_elapsed: float
    fens_per_second:float
    analyse_time_limit:float
    nodes_limit:int

#
# --- THIS LINE WAS THE ERROR AND HAS BEEN REMOVED ---
# from chessism_api.operations.models import PlayerCreateData, PlayerStatsCreateData # <-- NEW
#
    
# --- NEW: Pydantic model for creating PlayerStats ---
# This defines the data we expect to parse from the API
# and save to the database.

class StatsRecord(BaseModel):
    last: Optional[Dict[str, Any]] = None
    best: Optional[Dict[str, Any]] = None
    record: Optional[Dict[str, Any]] = None

class PuzzleRushStats(BaseModel):
    best: Optional[Dict[str, int]] = None

class TacticsStats(BaseModel):
    highest: Optional[Dict[str, int]] = None
    lowest: Optional[Dict[str, int]] = None

# This is the Pydantic model for creating/updating the DB
class PlayerStatsCreateData(BaseModel):
    player_name: str
    chess_rapid_last_rating: Optional[int] = None
    chess_rapid_best_rating: Optional[int] = None
    chess_rapid_games: Optional[int] = None
    chess_rapid_wins: Optional[int] = None
    chess_rapid_losses: Optional[int] = None
    chess_rapid_draws: Optional[int] = None
    # --- NEW ---
    chess_rapid_last_percentile: Optional[float] = None

    chess_blitz_last_rating: Optional[int] = None
    chess_blitz_best_rating: Optional[int] = None
    chess_blitz_games: Optional[int] = None
    chess_blitz_wins: Optional[int] = None
    chess_blitz_losses: Optional[int] = None
    chess_blitz_draws: Optional[int] = None
    # --- NEW ---
    chess_blitz_last_percentile: Optional[float] = None

    chess_bullet_last_rating: Optional[int] = None
    chess_bullet_best_rating: Optional[int] = None
    chess_bullet_games: Optional[int] = None
    chess_bullet_wins: Optional[int] = None
    chess_bullet_losses: Optional[int] = None
    chess_bullet_draws: Optional[int] = None
    # --- NEW ---
    chess_bullet_last_percentile: Optional[float] = None

    fide: Optional[int] = None
    puzzle_rush_best_score: Optional[int] = None
    tactics_highest_rating: Optional[int] = None
    tactics_lowest_rating: Optional[int] = None
    
    class Config:
        from_attributes = True # Allow reading from the DB model



### inner_api/operations/chess_com_api.py 
#chessism_api/operations/chess_com_api.py

import asyncio
import httpx
import json
import time
from typing import List, Tuple, Dict, Any, Optional
import pprint

# --- FIXED IMPORTS ---
import constants
from chessism_api.operations.models import PlayerCreateData
# ---

# --- API CLIENT FUNCTIONS ---

async def get_profile(player_name: str) -> Optional[PlayerCreateData]:
    """
    Fetches a player's profile from the Chess.com API.
    """
    PLAYER_URL = constants.PLAYER.replace('{player}', player_name)
    
    # --- FIX: Force HTTP/1.1 ---
    async with httpx.AsyncClient(timeout=5, http2=False) as client:
        try:
            response = await client.get(
                PLAYER_URL,
                headers={"User-Agent": constants.USER_AGENT}
            )
            response.raise_for_status()
            
            raw_data = response.json()

            # --- Transformation to match PlayerCreateData Pydantic model ---
            processed_data = {} 
            processed_data['player_name'] = player_name.lower()

            processed_data['name'] = raw_data.get('name')
            processed_data['url'] = raw_data.get('url')
            processed_data['title'] = raw_data.get('title')
            processed_data['avatar'] = raw_data.get('avatar')
            processed_data['followers'] = raw_data.get('followers')
            
            country_url = raw_data.get('country')
            if country_url:
                processed_data['country'] = country_url.split('/')[-1]
            else:
                processed_data['country'] = None

            processed_data['location'] = raw_data.get('location')
            
            joined_ts = raw_data.get('joined')
            if joined_ts is not None:
                try:
                    processed_data['joined'] = int(joined_ts)
                except (ValueError, TypeError):
                    print(f"Warning: Could not convert 'joined' ({joined_ts}) to int for {player_name}. Setting to 0.")
                    processed_data['joined'] = 0
            else:
                processed_data['joined'] = 0 # Default to 0 if joined is None
            
            processed_data['status'] = raw_data.get('status')
            processed_data['is_streamer'] = raw_data.get('is_streamer')
            processed_data['twitch_url'] = raw_data.get('twitch_url')
            processed_data['verified'] = raw_data.get('verified')
            processed_data['league'] = raw_data.get('league')
            
            # Use Pydantic to validate and create the data object
            player_data = PlayerCreateData(**processed_data)
            return player_data # Return the Pydantic model instance

        except httpx.HTTPStatusError as e:
            if e.response.status_code == 404:
                print(f"Player '{player_name}' not found on Chess.com (404).")
                return None
            print(f"HTTP error for profile {player_name}: {e.response.status_code} - {e.response.text}")
            return None
        except httpx.RequestError as e:
            # --- UPDATED: Use repr(e) for better error details ---
            print(f"Request error for profile {player_name}: {repr(e)}")
            return None
        except Exception as e:
            print(f"An unexpected error occurred getting profile {player_name}: {e}")
            return None


# --- NEW FUNCTION: get_player_stats ---
async def get_player_stats(player_name: str) -> Optional[Dict[str, Any]]:
    """
    Fetches a player's stats from the Chess.com API.
    Returns the raw JSON dictionary.
    """
    STATS_URL = constants.STATS.replace('{username}', player_name)
    
    async with httpx.AsyncClient(timeout=5, http2=False) as client:
        try:
            response = await client.get(
                STATS_URL,
                headers={"User-Agent": constants.USER_AGENT}
            )
            response.raise_for_status()
            raw_data = response.json()
            return raw_data

        except httpx.HTTPStatusError as e:
            if e.response.status_code == 404:
                print(f"Stats for '{player_name}' not found on Chess.com (404).")
                return None
            print(f"HTTP error for stats {player_name}: {e.response.status_code} - {e.response.text}")
            return None
        except httpx.RequestError as e:
            print(f"Request error for stats {player_name}: {repr(e)}")
            return None
        except Exception as e:
            print(f"An unexpected error occurred getting stats {player_name}: {e}")
            return None


async def ask_twice(player_name: str, year: int, month: int, client: httpx.AsyncClient) -> Optional[httpx.Response]:
    """
    Fetches game archives for a specific month, with a retry logic.
    """
    month_str = f"{month:02d}"

    DOWNLOAD_MONTH_URL = (
        constants.DOWNLOAD_MONTH
        .replace("{player}", player_name)
        .replace("{year}", str(year))
        .replace("{month}", month_str)
    )

    try:
        games_response = await client.get(
            DOWNLOAD_MONTH_URL,
            follow_redirects=True,
            timeout=5,
            headers={"User-Agent": constants.USER_AGENT}
        )
        
        if not games_response.content:
            await asyncio.sleep(1)
            games_response = await client.get(
                DOWNLOAD_MONTH_URL,
                follow_redirects=True,
                timeout=10,
                headers={"User-Agent": constants.USER_AGENT}
            )
        
        if not games_response.content:
            print(f"No content after two attempts for {player_name} in {year}-{month_str}.")
            return None

        games_response.raise_for_status()
        return games_response

    except httpx.HTTPStatusError as e:
        if e.response.status_code == 404:
            print(f"No games found for {player_name} in {year}-{month_str} (404).")
            return None
        print(f"HTTP error downloading month {year}-{month_str}: {e.response.status_code} - {e.response.text}")
        return None
    except httpx.RequestError as e:
        print(f"Request error downloading month {year}-{month_str}: {e}")
        return None
    except Exception as e:
        print(f"An unexpected error occurred downloading month {year}-{month_str}: {e}")
        return None


async def download_month(player_name: str, year: int, month: int, client: httpx.AsyncClient) -> Optional[httpx.Response]:
    """
    Wrapper for ask_twice to get a month's games, passing the shared client.
    """
    games = await ask_twice(player_name, year, month, client)
    return games


async def month_of_games(param: Dict[str, Any], client: httpx.AsyncClient) -> Optional[Dict[str, Any]]:
    """
    Downloads a month of games and returns as parsed JSON dictionary.
    """
    player_name = param["player_name"]
    year = param["year"]
    month = param["month"]

    pgn_response = await download_month(player_name, year, month, client)
    
    if pgn_response is None:
        return None

    try:
        text_games = pgn_response.text
        text_games = text_games.replace(' \\"Let"s Play!','lets_play') 
        
        parsed_json = json.loads(text_games)
        return parsed_json
    except json.JSONDecodeError as e:
        print(f'JSON decoding failed for year: {year}, month: {month}: {e}')
        return None
    except Exception as e:
        print(f"Error filtering raw PGN for {year}-{month}: {e}")
        return None


async def download_months(
                        player_name: str,
                        valid_dates: List[str],
                        # --- REMOVED: Concurrency parameters ---
                        min_delay_between_requests: float = 0.5
                        ) -> Dict[int, Dict[int, List[Dict[str, Any]]]]:
    """
    --- SERIAL VERSION ---
    Downloads games for a player's month strings one by one
    to comply with Chess.com policies.
    """
    all_games_by_month: Dict[int, Dict[int, List[Dict[str, Any]]]] = {}
    
    # --- MODIFIED: Removed Semaphore ---
    
    async with httpx.AsyncClient(timeout=15, http2=False) as shared_client:
        
        print(f"Starting serial download of {len(valid_dates)} months...")
        start_time = time.time()
        
        # --- MODIFIED: Use a simple for loop instead of asyncio.gather ---
        for month_str in valid_dates:
            await asyncio.sleep(min_delay_between_requests) # Respect rate limit

            year_str, month_str_val = month_str.split('-')
            year = int(year_str)
            month = int(month_str_val)
            param = {"player_name": player_name, "year": year, "month": month}
            
            try:
                result = await month_of_games(param, shared_client) 

                if result is None:
                    print(f"No data for {year}-{month}.")
                    continue
                
                if 'games' in result and result['games'] is not None:
                    if year not in all_games_by_month:
                        all_games_by_month[year] = {}
                    all_games_by_month[year][month] = result['games']
                else:
                    print(f"No games or invalid data for {year}-{month} (missing/empty 'games' key in parsed JSON).")

            except Exception as e:
                print(f"An error occurred processing {month_str}: {e}")
        
        end_time = time.time()
        print(f"Finished downloading {len(valid_dates)} months in {end_time - start_time:.2f} seconds.")

    return all_games_by_month



### inner_api/operations/analysis.py 
# chessism_api/operations/analysis.py

# chessism_api/operations/analysis.py

import asyncio
import httpx
import time
from typing import List, Dict, Any, Optional
from sqlalchemy.ext.asyncio import AsyncSession

# --- FIXED IMPORTS ---
from chessism_api.database.engine import AsyncDBSession
from chessism_api.database.models import Fen
from chessism_api.database.db_interface import DBInterface
from chessism_api.database.ask_db import (
    get_fens_for_analysis,
    get_player_fens_for_analysis
)
# ---

# --- RESTORED DUAL-GPU URLS ---
LEELA_URLS = {
    0: "http://leela-service-gpu0:9999/analyze", # Port 9999
    1: "http://leela-service-gpu1:9999/analyze"  # Mapped to host 9998
}

# Initialize the DBInterface for Fen
fen_interface = DBInterface(Fen)

async def _call_leela_service(
    client: httpx.AsyncClient,
    url: str,
    fens: List[str],
    nodes: int
) -> Optional[List[Dict[str, Any]]]:
    """
    Sends a batch of FENs to the specified Leela service.
    """
    payload = {
        "fens": fens,
        "nodes_limit": nodes
    }
    try:
        response = await client.post(url, json=payload, timeout=None) # Disable timeout
        response.raise_for_status()
        return response.json()
    except httpx.HTTPStatusError as e:
        print(f"HTTP error calling Leela service: {e.response.status_code} - {e.response.text}", flush=True)
        return None
    except httpx.RequestError as e:
        print(f"Request error calling Leela service: {repr(e)}", flush=True)
        return None
    except Exception as e:
        print(f"Unexpected error in _call_leela_service: {repr(e)}", flush=True)
        return None

def _format_leela_results(
    leela_output: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    Parses the raw JSON output from Leela into the DB format.
    """
    formatted_results = []
    for item in leela_output:
        if not item or not item.get("is_valid"):
            continue

        fen_str = item.get("fen")
        analysis = item.get("analysis", {})
        
        # Extract score (in centipawns)
        # We use .get('score') which is the simplified PovScore
        score_cp = analysis.get("score")

        # Extract next_moves (the principal variation)
        pv = analysis.get("pv", [])
        
        # Convert PV list of moves to a single string
        next_moves_str = " ".join(pv) if pv else None

        if fen_str and score_cp is not None:
            formatted_results.append({
                "fen": fen_str,
                "score": float(score_cp),
                "next_moves": next_moves_str
            })
    return formatted_results

async def run_analysis_job(
    gpu_index: int,
    total_fens_to_process: int,
    batch_size: int,
    nodes_limit: int
):
    """
    The main background task for a general analysis job.
    """
    job_id = f"GPU-{gpu_index}"
    # This will now correctly select the URL
    leela_url = LEELA_URLS[gpu_index] 
    
    print(f"--- [START JOB {job_id}] ---", flush=True)
    print(f"Targeting {total_fens_to_process} FENs, Batch Size: {batch_size}, Nodes: {nodes_limit}", flush=True)
    print(f"[{job_id}] Routing to: {leela_url}", flush=True) # <-- Test log

    total_processed = 0
    total_failed_batches = 0
    job_start_time = time.time()
    
    # Use one httpx client for the life of the job
    async with httpx.AsyncClient() as client:
        while total_processed < total_fens_to_process:
            current_batch_size = min(batch_size, total_fens_to_process - total_processed)
            if current_batch_size <= 0:
                break
                
            print(f"[{job_id}] Processing batch { (total_processed // batch_size) + 1 }...", flush=True)
            
            fens_to_process = None
            session: Optional[AsyncSession] = None
            
            try:
                # 1. Start a transaction and get FENs (with lock)
                # --- THIS IS THE FIX for the TypeError ---
                session, fens_to_process = await get_fens_for_analysis(current_batch_size)
                
                if not fens_to_process or session is None:
                    print(f"[{job_id}] No more FENs found to analyze. Stopping job.", flush=True)
                    break
                
                # --- MODIFIED: Start timing ---
                batch_start_time = time.time()

                # 2. Call Leela service
                leela_results = await _call_leela_service(
                    client,
                    leela_url,
                    fens_to_process,
                    nodes_limit
                )
                
                # --- MODIFIED: End timing ---
                batch_end_time = time.time()
                batch_duration = batch_end_time - batch_start_time
                
                if not leela_results:
                    print(f"[{job_id}] Failed to get results from Leela service. Skipping batch.", flush=True)
                    total_failed_batches += 1
                    # Rollback to release the locks
                    await session.rollback()
                    continue

                # 3. Format results
                db_ready_data = _format_leela_results(leela_results)
                
                if not db_ready_data:
                    print(f"[{job_id}] No valid analysis data returned from Leela. Skipping batch.", flush=True)
                    total_failed_batches += 1
                    # Rollback to release the locks
                    await session.rollback()
                    continue

                # 4. Save results (using the same session)
                await fen_interface.update_fen_analysis_data(session, db_ready_data)
                
                # 5. Commit the transaction
                # This saves the data AND releases the 'FOR UPDATE SKIP LOCKED'
                await session.commit()
                
                # --- MODIFIED: Calculate time per FEN ---
                fens_in_batch = len(fens_to_process)
                time_per_fen = (batch_duration / fens_in_batch) if fens_in_batch > 0 else 0
                total_processed += fens_in_batch
                
                print(f"[{job_id}] Batch complete. Total FENs: {total_processed}. Batch Time: {batch_duration:.2f}s ({time_per_fen:.2f} s/FEN)", flush=True)

            except Exception as e:
                print(f"CRITICAL: Unhandled error in {job_id} analysis loop: {repr(e)}", flush=True)
                total_failed_batches += 1
                if session:
                    await session.rollback() # Ensure locks are released on failure
            finally:
                if session:
                    await session.close()
            
            # Small delay to prevent spamming
            await asyncio.sleep(1) 

    job_end_time = time.time()
    print(f"--- [END JOB {job_id}] ---", flush=True)
    print(f"Total time: {(job_end_time - job_start_time):.2f} seconds", flush=True)
    print(f"Total FENs processed: {total_processed}", flush=True)
    print(f"Total failed batches: {total_failed_batches}", flush=True)

async def run_player_analysis_job(
    player_name: str,
    gpu_index: int,
    total_fens_to_process: int,
    batch_size: int,
    nodes_limit: int
):
    """
    The main background task for a player-specific analysis job.
    """
    job_id = f"GPU-{gpu_index} (Player: {player_name})"
    leela_url = LEELA_URLS[gpu_index]
    
    print(f"--- [START JOB {job_id}] ---", flush=True)
    print(f"Targeting {total_fens_to_process} FENs, Batch Size: {batch_size}, Nodes: {nodes_limit}", flush=True)
    print(f"[{job_id}] Routing to: {leela_url}", flush=True) # <-- Test log

    total_processed = 0
    total_failed_batches = 0
    job_start_time = time.time()

    async with httpx.AsyncClient() as client:
        while total_processed < total_fens_to_process:
            current_batch_size = min(batch_size, total_fens_to_process - total_processed)
            if current_batch_size <= 0:
                break
                
            print(f"[{job_id}] Processing batch { (total_processed // batch_size) + 1 }...", flush=True)
            
            fens_to_process = None
            session: Optional[AsyncSession] = None
            
            try:
                # 1. Start a transaction and get player-specific FENs (with lock)
                session, fens_to_process = await get_player_fens_for_analysis(
                    player_name,
                    current_batch_size
                )
                
                if not fens_to_process or session is None:
                    print(f"[{job_id}] No more FENs found for player {player_name}. Stopping job.", flush=True)
                    break
                
                # --- MODIFIED: Start timing ---
                batch_start_time = time.time()

                # 2. Call Leela service
                leela_results = await _call_leela_service(
                    client,
                    leela_url,
                    fens_to_process,
                    nodes_limit
                )
                
                # --- MODIFIED: End timing ---
                batch_end_time = time.time()
                batch_duration = batch_end_time - batch_start_time
                
                if not leela_results:
                    print(f"[{job_id}] Failed to get results from Leela. Skipping batch.", flush=True)
                    total_failed_batches += 1
                    await session.rollback()
                    continue

                # 3. Format results
                db_ready_data = _format_leela_results(leela_results)
                
                if not db_ready_data:
                    print(f"[{job_id}] No valid analysis data from Leela. Skipping batch.", flush=True)
                    total_failed_batches += 1
                    await session.rollback()
                    continue

                # 4. Save results (using the same session)
                await fen_interface.update_fen_analysis_data(session, db_ready_data)
                
                # 5. Commit the transaction
                await session.commit()
                
                # --- MODIFIED: Calculate time per FEN ---
                fens_in_batch = len(fens_to_process)
                time_per_fen = (batch_duration / fens_in_batch) if fens_in_batch > 0 else 0
                total_processed += fens_in_batch

                print(f"[{job_id}] Batch complete. Total FENs: {total_processed}. Batch Time: {batch_duration:.2f}s ({time_per_fen:.2f} s/FEN)", flush=True)

            except Exception as e:
                print(f"CRITICAL: Unhandled error in {job_id} loop: {repr(e)}", flush=True)
                total_failed_batches += 1
                if session:
                    await session.rollback()
            finally:
                if session:
                    await session.close()
            
            await asyncio.sleep(1)

    job_end_time = time.time()
    print(f"--- [END JOB {job_id}] ---", flush=True)
    print(f"Total time: {(job_end_time - job_start_time):.2f} seconds", flush=True)
    print(f"Total FENs processed: {total_processed}", flush=True)
    print(f"Total failed batches: {total_failed_batches}", flush=True)



### inner_api/operations/check_player_in_db.py 
# chessism_api/operations/check_player_in_db.py

import time
from typing import Set, List
import asyncio
from math import ceil

from sqlalchemy import Column, String, text, select
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import declarative_base

# --- FIXED IMPORTS ---
from chessism_api.operations.models import PlayerCreateData
from chessism_api.database.db_interface import DBInterface
from chessism_api.database.models import Player
# ---

# --- TEMPORARY TABLE MODEL ---
TempBase = declarative_base()

class TempPlayerName(TempBase):
    """
    SQLAlchemy model for a temporary table to hold player names for bulk lookups.
    """
    __tablename__ = "temp_player_names_check" # Use a distinct name
    # ON COMMIT DROP is good, but means everything must be in one transaction.
    __table_args__ = {'prefixes': ['TEMPORARY']}

    player_name_col = Column(String, primary_key=True)


# --- ASYNC FUNCTION TO GET ONLY NEW PLAYERS ---
async def get_only_players_not_in_db(player_names: Set[str]) -> Set[str]:
    """
    Identifies which player names from the input set do not yet exist in the Player table.
    Uses a temporary table to efficiently handle large numbers of player names.
    """
    player_interface = DBInterface(Player)
    if not player_names:
        print("No player names provided for lookup.")
        return set()

    player_names_list = list(player_names)
    INSERT_VALUES_BATCH_SIZE = 1000
    players_found_in_db = set()

    # --- FIX: Use .get_session() from the interface ---
    async with player_interface.get_session() as session:
        start_temp_table_ops = time.time()
        print(f"[{time.time()-start_temp_table_ops:.2f}s] Starting temp table operations for {len(player_names)} players...")

        try:
            # Step 1: Create the temporary table.
            # Use a unique table name to avoid conflicts if this runs concurrently.
            temp_table_name = "temp_player_names_check"
            await session.execute(text(f"""
                CREATE TEMPORARY TABLE IF NOT EXISTS {temp_table_name} (
                    player_name_col VARCHAR PRIMARY KEY
                ) ON COMMIT DROP;
            """))
            print(f"[{time.time()-start_temp_table_ops:.2f}s] Temporary table created.")

            # Step 2: Insert player names into the temporary table in batches.
            for i in range(0, len(player_names_list), INSERT_VALUES_BATCH_SIZE):
                batch = player_names_list[i : i + INSERT_VALUES_BATCH_SIZE]

                # --- SYNTAX ERROR FIX ---
                # Rewritten to avoid the complex f-string list comprehension
                # that was causing the SyntaxError.
                values_to_insert: List[str] = []
                for name in batch:
                    # Escape single quotes for SQL
                    safe_name = name.replace("'", "''")
                    values_to_insert.append(f"('{safe_name}')")
                
                values_clause = ", ".join(values_to_insert)
                # --- END FIX ---
                
                if values_clause: # Ensure batch wasn't empty
                    insert_sql = f"INSERT INTO {temp_table_name} (player_name_col) VALUES {values_clause} ON CONFLICT DO NOTHING;"
                    await session.execute(text(insert_sql))

            print(f"[{time.time()-start_temp_table_ops:.2f}s] All {len(player_names_list)} player names inserted into temp table.")

            # Step 3: Query the main player table by joining with the temporary table.
            start_join_query = time.time()
            print(f"[{time.time()-start_temp_table_ops:.2f}s] Performing JOIN query to find existing players...")
            
            # Use raw SQL for the join as the temp table is not in SQLAlchemy metadata
            join_sql = f"""
                SELECT p.player_name 
                FROM player AS p
                JOIN {temp_table_name} AS t ON p.player_name = t.player_name_col;
            """
            
            result = await session.execute(text(join_sql))
            players_found_in_db.update(result.scalars().all())
            print(f"[{time.time()-start_temp_table_ops:.2f}s] JOIN query completed in {time.time()-start_join_query:.2f}s.")
        
        except Exception as e:
            await session.rollback()
            print(f"Error during temp table player check: {e}")
            raise # Re-raise the exception
        
        # Session auto-commits here, and 'ON COMMIT DROP' cleans up the temp table.

    print(f"Total time for get_only_players_not_in_db: {time.time()-start_temp_table_ops:.2f} seconds")
    return player_names - players_found_in_db

