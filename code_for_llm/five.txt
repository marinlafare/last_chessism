
            PART FIVE: CHESSISM_API DATABASE
            This is the fifth part, it contains the 'database' part of the chessism_api:
            
            


### inner_api/database/models.py 
#chessism_api/database/models.py

from typing import Any, Dict
from sqlalchemy import (
    Column, ForeignKey, Integer, String, Float, BigInteger, Table,
    DateTime, func, UniqueConstraint
)
from sqlalchemy.orm import declarative_base, relationship
from sqlalchemy.types import Boolean

Base = declarative_base()


def to_dict(obj: Base) -> Dict[str, Any]:
    """
    Serializes a SQLAlchemy ORM object into a dictionary.
    """
    return {c.name: getattr(obj, c.name) for c in obj.__table__.columns}

# --- REMOVED: Base.to_dict = to_dict ---
# This "monkey-patching" can interfere with SQLAlchemy's
# mapper configuration and relationship loading.


class Player(Base):
    __tablename__ = "player"
    player_name = Column("player_name", String, primary_key=True, nullable=False, unique=True)
    name = Column('name', String, nullable=True)
    url = Column('url', String, nullable=True)
    title = Column('title', String, nullable=True)
    avatar = Column('avatar', String, nullable=True)
    followers = Column('followers', Integer,nullable=True)
    country = Column('country', String, nullable=True)
    location = Column('location', String, nullable=True)
    joined = Column('joined', BigInteger, nullable=True) # Use BigInteger for Unix timestamps
    status = Column('status', String, nullable=True)
    is_streamer = Column('is_streamer', Boolean, nullable=True)
    twitch_url = Column('twitch_url', String, nullable=True)
    verified = Column('verified', Boolean, nullable=True)
    league = Column('league', String, nullable=True)
    
    stats = relationship(
        "PlayerStats", 
        back_populates="player", 
        uselist=False, # Signifies a one-to-one relationship
        cascade="all, delete-orphan"
    )

class Game(Base):
    __tablename__ = 'game'
    link = Column('link',BigInteger, primary_key = True, unique = True)
    white = Column("white", String, ForeignKey("player.player_name"), nullable=False)
    black = Column("black", String, ForeignKey("player.player_name"), nullable=False)

    year = Column("year", Integer, nullable=False)
    month = Column("month", Integer, nullable=False)
    day = Column("day", Integer, nullable=False)
    hour = Column("hour", Integer, nullable=False)
    minute = Column("minute", Integer, nullable=False)
    second = Column("second", Integer, nullable=False)
        
    white_elo = Column("white_elo", Integer, nullable=False)
    black_elo = Column("black_elo", Integer, nullable=False)
    white_result = Column("white_result", Float, nullable=False)
    black_result = Column("black_result", Float, nullable=False)
    white_str_result = Column("white_str_result", String, nullable=False)
    black_str_result = Column("black_str_result", String, nullable=False)
    time_control = Column("time_control", String, nullable=False)
    eco = Column("eco", String, nullable=False)
    
    time_elapsed = Column("time_elapsed", Float, nullable=False)
    
    n_moves = Column("n_moves", Integer, nullable=False)
    fens_done = Column('fens_done', Boolean, nullable = False)
    
    white_player = relationship(Player, foreign_keys=[white])
    black_player = relationship(Player, foreign_keys=[black])
    fens = relationship(
        'Fen',
        secondary='game_fen_association',
        back_populates='games'
    )

class Month(Base):
    __tablename__ = "months"
    id = Column(Integer, primary_key=True, autoincrement=True)
    
    player_name = Column("player_name",
                         String,
                         ForeignKey("player.player_name"),
                         primary_key = False,
                         unique=False,
                         nullable=False)
    year = Column("year", Integer, nullable=False, unique=False)
    month = Column("month", Integer, nullable=False, unique=False)
    n_games = Column("n_games",Integer, nullable=False, unique=False)
    
    player = relationship(Player, foreign_keys=[player_name])
    
    __table_args__ = (
        UniqueConstraint('player_name', 'year', 'month', name='_player_year_month_uc'),
    )


class Move(Base):
    __tablename__ = "moves"
    id = Column(Integer, primary_key=True, autoincrement=True)
    link = Column("link",BigInteger,ForeignKey("game.link"),
                  nullable=False,unique=False)
    n_move = Column("n_move", Integer, nullable=False)
    white_move = Column("white_move", String, nullable=False)
    black_move = Column("black_move", String, nullable=False)
    white_reaction_time = Column("white_reaction_time", Float, nullable=False)
    black_reaction_time = Column("black_reaction_time", Float, nullable=False)
    white_time_left = Column("white_time_left", Float, nullable=False)
    black_time_left = Column("black_time_left", Float, nullable=False)
    
    game = relationship(Game, foreign_keys=[link])
    
    __table_args__ = (
        UniqueConstraint('link', 'n_move', name='_game_link_move_num_uc'),
    )


class Fen(Base):
    __tablename__ = "fen"
    fen = Column('fen',String, primary_key = True, index = True, unique = True)
    n_games = Column('n_games',BigInteger, nullable = False)
    moves_counter = Column('moves_counter',String, nullable = False)
    next_moves = Column('next_moves',String, nullable = True)
    score = Column('score', Float, nullable = True)
    
    games = relationship(
        'Game',
        secondary='game_fen_association',
        back_populates='fens'
    )

game_fen_association = Table(
    'game_fen_association', Base.metadata,
    Column('game_link', BigInteger, ForeignKey('game.link'), primary_key=True),
    Column('fen_fen', String, ForeignKey('fen.fen'), primary_key=True)
)

class AnalysisTimes(Base):
    __tablename__ = "analysis_times"
    id = Column(Integer, primary_key=True, autoincrement=True)
    batch_index = Column('batch_index',Integer, nullable=False, unique=False)
    n_batches = Column('n_batches',Integer, nullable=False, unique=False)
    card = Column('card', Integer,nullable=False, unique=False)
    model = Column('model',String,nullable=False, unique=False)
    n_fens = Column('n_fens',Integer, nullable=False, unique=False)
    time_elapsed=Column('time_elapsed', Float,nullable=False, unique=False)
    fens_per_second = Column('fens_per_second', Float, nullable=False,unique=False)
    analyse_time_limit = Column('analyse_time_limit', Float, nullable=False,unique=False)
    nodes_limit = Column('nodes_limit', Integer, nullable=False, unique=False)


class PlayerStats(Base):
    __tablename__ = "player_stats"
    
    player_name = Column(
        String, 
        ForeignKey("player.player_name", ondelete="CASCADE"), 
        primary_key=True
    )
    
    last_updated = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())
    
    player = relationship("Player", back_populates="stats")

    # --- Rapid ---
    chess_rapid_last_rating = Column(Integer, nullable=True)
    chess_rapid_best_rating = Column(Integer, nullable=True)
    chess_rapid_games = Column(Integer, nullable=True)
    chess_rapid_wins = Column(Integer, nullable=True)
    chess_rapid_losses = Column(Integer, nullable=True)
    chess_rapid_draws = Column(Integer, nullable=True)
    # --- NEW: Percentile ---
    chess_rapid_last_percentile = Column(Float, nullable=True)

    # --- Blitz ---
    chess_blitz_last_rating = Column(Integer, nullable=True)
    chess_blitz_best_rating = Column(Integer, nullable=True)
    chess_blitz_games = Column(Integer, nullable=True)
    chess_blitz_wins = Column(Integer, nullable=True)
    chess_blitz_losses = Column(Integer, nullable=True)
    chess_blitz_draws = Column(Integer, nullable=True)
    # --- NEW: Percentile ---
    chess_blitz_last_percentile = Column(Float, nullable=True)

    # --- Bullet ---
    chess_bullet_last_rating = Column(Integer, nullable=True)
    chess_bullet_best_rating = Column(Integer, nullable=True)
    chess_bullet_games = Column(Integer, nullable=True)
    chess_bullet_wins = Column(Integer, nullable=True)
    chess_bullet_losses = Column(Integer, nullable=True)
    chess_bullet_draws = Column(Integer, nullable=True)
    # --- NEW: Percentile ---
    chess_bullet_last_percentile = Column(Float, nullable=True)

    # --- Other ---
    fide = Column(Integer, nullable=True)
    puzzle_rush_best_score = Column(Integer, nullable=True)
    tactics_highest_rating = Column(Integer, nullable=True)
    tactics_lowest_rating = Column(Integer, nullable=True)



### inner_api/database/db_interface.py 
#chessism_api/database/db_interface.py

import os
from typing import Any, List, Dict, TypeVar
# --- MODIFIED: Import 'text' for raw SQL in the update ---
from sqlalchemy import select, insert, Integer, func, update, bindparam, text
from sqlalchemy.orm import Session, joinedload
from sqlalchemy.ext.asyncio import AsyncSession

# --- FIXED IMPORTS ---
from chessism_api.database.engine import AsyncDBSession
from chessism_api.database.models import Base, Fen, to_dict, Game, game_fen_association
# ---

from sqlalchemy.dialects.postgresql import insert as pg_insert
from datetime import datetime, timezone

_ModelType = TypeVar("_ModelType", bound=Base)

DataObject = Dict[str, Any]
ListOfDataObjects = List[DataObject]

class DBInterface:
    def __init__(self, db_class: TypeVar('_ModelType', bound=Base)):
        self.db_class = db_class

    async def create(self, data: DataObject) -> DataObject:
        """
        Creates a single new record.
        """
        async with AsyncDBSession() as session:
            try:
                item: _ModelType = self.db_class(**data)
                session.add(item)
                await session.commit()
                await session.refresh(item)
                result = to_dict(item)
                return result
            except Exception as e:
                await session.rollback()
                raise

    async def read(self, **filters) -> ListOfDataObjects:
        """
        Reads records from the database based on filters.
        Returns a list of dictionaries.
        """
        async with AsyncDBSession() as session:
            try:
                stmt = select(self.db_class).filter_by(**filters)
                result = await session.execute(stmt)
                return [to_dict(row) for row in result.scalars().all()]
            except Exception as e:
                raise

    async def update(self, primary_key_value: Any, data: DataObject) -> DataObject | None:
        """
        Updates an existing record identified by its primary key.
        """
        async with AsyncDBSession() as session:
            try:
                item: _ModelType | None = await session.get(self.db_class, primary_key_value)
                if item is None:
                    return None
                for key, value in data.items():
                    if hasattr(item, key):
                        setattr(item, key, value)
                await session.commit()
                await session.refresh(item)
                return to_dict(item)
            except Exception as e:
                await session.rollback()
                raise

    async def delete(self, primary_key_value: Any) -> DataObject | None:
        """
        Deletes a record identified by its primary key.
        """
        async with AsyncDBSession() as session:
            try:
                item: _ModelType | None = await session.get(self.db_class, primary_key_value)
                if item is None:
                    return None
                result = to_dict(item)
                await session.delete(item)
                await session.commit()
                return result
            except Exception as e:
                await session.rollback()
                raise

    def get_session(self):
        """Returns an AsyncDBSession context manager."""
        return AsyncDBSession()

    async def create_all(self, data: ListOfDataObjects) -> bool:
        """
        Inserts multiple records in a NEW session.
        Handles specific UPSERT logic for Fen.
        """
        async with AsyncDBSession() as session:
            try:
                # Use the new helper method, passing the session
                await self.create_all_with_session(session, data)
                await session.commit()
                return True
            except Exception as e:
                await session.rollback()
                raise

    # --- NEW METHOD ---
    async def create_all_with_session(self, session: AsyncSession, data: ListOfDataObjects) -> bool:
        """
        Inserts multiple records using an EXISTING session.
        This is used for atomic transactions.
        """
        if not data:
            return True

        if self.db_class == Fen:
            params_per_row = 5 # 'fen', 'n_games', 'moves_counter', 'score', 'next_moves'
        else:
            params_per_row = len(self.db_class.__table__.columns) if hasattr(self.db_class, '__table__') else 3

        INSERT_BATCH_SIZE = 5000
        if params_per_row > 0:
            effective_batch_size = min(INSERT_BATCH_SIZE, 32000 // params_per_row)
            if effective_batch_size == 0:
                effective_batch_size = 1
        else:
            effective_batch_size = INSERT_BATCH_SIZE

        chunks = [data[i:i + effective_batch_size] for i in range(0, len(data), effective_batch_size)]
        
        valid_columns = {c.name for c in self.db_class.__table__.columns}
        
        # This function now uses the *provided* session and does NOT commit.
        for i, chunk in enumerate(chunks):
            if not chunk:
                continue
            
            clean_chunk = []
            for data_dict in chunk:
                clean_dict = {
                    k: v for k, v in data_dict.items() 
                    if k in valid_columns
                }
                clean_chunk.append(clean_dict)
            
            if not clean_chunk:
                continue 
            
            if self.db_class == Fen:
                stmt = pg_insert(self.db_class).values(clean_chunk).on_conflict_do_update(
                    index_elements=[self.db_class.fen],
                    set_={
                        'n_games': (self.db_class.n_games.cast(Integer) + pg_insert(self.db_class).excluded.n_games.cast(Integer)),
                        'moves_counter': text(
                            "CASE WHEN position(excluded.moves_counter in fen.moves_counter) > 0 "
                            "THEN fen.moves_counter "
                            "ELSE (fen.moves_counter || excluded.moves_counter) END"
                        )
                    }
                )
                await session.execute(stmt)
            else:
                await session.run_sync(
                    lambda sync_session, c=clean_chunk: sync_session.bulk_insert_mappings(self.db_class, c)
                )
        return True
    # --- END NEW METHOD ---


    async def upsert_main_fens(self,
                                   objects_to_insert: ListOfDataObjects,
                                   objects_to_update: ListOfDataObjects) -> bool:
        """
        PERFORMANCE WARNING: The update logic (for item in objects_to_update)
        runs session.get() inside a loop. This is an N+1 query problem
        and will be very slow for large update lists.
        ---
        """
        if not objects_to_insert and not objects_to_update:
            return True

        async with AsyncDBSession() as session:
            try:
                # --- Process Inserts ---
                if objects_to_insert:
                    insert_stmt = pg_insert(Fen).values(objects_to_insert).on_conflict_do_nothing(
                        index_elements=[Fen.fen]
                    )
                    await session.execute(insert_stmt)

                # --- Process Updates (N+1 Query Problem) ---
                if objects_to_update:
                    for item_data in objects_to_update:
                        fen_to_update = item_data['fen']
                        new_moves_counter = item_data['moves_counter']
                        
                        # This session.get() is inside a loop, causing N+1 queries.
                        db_item = await session.get(Fen, fen_to_update)
                        
                        if db_item:
                            existing_moves_counter = db_item.moves_counter
                            if new_moves_counter not in existing_moves_counter:
                                updated_moves_counter = existing_moves_counter + new_moves_counter
                            else:
                                updated_moves_counter = existing_moves_counter
                                
                            db_item.n_games += item_data['n_games']
                            db_item.moves_counter = updated_moves_counter
                            db_item.next_moves = item_data['next_moves']
                            db_item.score = item_data['score']

                await session.commit()
                return True
            except Exception as e:
                await session.rollback()
                raise

    async def associate_fen_with_games(self, associations_to_insert_raw: List[Dict[str, Any]]) -> bool:
        """
        Associates multiple FENs with their respective lists of games in a bulk operation
        by directly inserting into the association table.

        Args:
            associations_to_insert_raw: A list of dictionaries, e.g.:
                [{"game_link": 123, "fen_fen": "r1bqk..."},
                 {"game_link": 124, "fen_fen": "r1b1k..."}]
        """
        if not associations_to_insert_raw:
            print("No valid associations to insert after processing input data.")
            return True

        params_per_row = 2
        INSERT_BATCH_SIZE = 5000
        effective_batch_size = min(INSERT_BATCH_SIZE, 32000 // params_per_row)
        if effective_batch_size == 0:
            effective_batch_size = 1

        chunks = [associations_to_insert_raw[i:i + effective_batch_size]
                  for i in range(0, len(associations_to_insert_raw), effective_batch_size)]

        async with AsyncDBSession() as session:
            try:
                total_inserted_rows = 0
                for i, chunk in enumerate(chunks):
                    if not chunk:
                        continue

                    insert_stmt = pg_insert(game_fen_association).values(chunk).on_conflict_do_nothing(
                        index_elements=[game_fen_association.c.game_link, game_fen_association.c.fen_fen]
                    )
                    result = await session.execute(insert_stmt)
                    total_inserted_rows += result.rowcount

                await session.commit()
                print(f"Successfully committed a total of {total_inserted_rows} new associations.")
                return True

            except Exception as e:
                await session.rollback()
                print(f"An error occurred during bulk FEN-Game association: {e}")
                raise

    async def update_all(self, data: ListOfDataObjects) -> bool:
        """
        Updates multiple Game records to set 'fens_done' = True.
        
        Args:
            data: A list of game links (Integers) to be updated.
        """
        if not data:
            print(f"No data provided for bulk update of {self.db_class.__tablename__}.")
            return True

        primary_key_column = Game.link
        links_to_update = data
        BATCH_SIZE = 10000

        chunks = [links_to_update[i:i + BATCH_SIZE] for i in range(0, len(links_to_update), BATCH_SIZE)]

        async with AsyncDBSession() as session:
            try:
                total_updated_rows = 0
                for i, chunk in enumerate(chunks):
                    if not chunk:
                        continue

                    stmt = (
                        update(Game)
                        .where(primary_key_column.in_(chunk))
                        .values(fens_done=True)
                    )
                    
                    result = await session.execute(stmt)
                    total_updated_rows += result.rowcount

                await session.commit()
                print(f"Successfully committed a total of {total_updated_rows} game updates for 'fens_done'.")
                return True

            except Exception as e:
                await session.rollback()
                print(f"An error occurred during bulk update of game 'fens_done': {e}")
                raise
                
    async def update_fen_analysis_data(self,
                                           session: AsyncSession, # <-- MODIFIED: Use existing session
                                           analysis_data: ListOfDataObjects) -> int:
        """
        Updates 'score' and 'next_moves' for existing Fen records in a bulk operation
        using an EXISTING session (for transactional safety with FOR UPDATE).
        
        Args:
            session: The active AsyncSession holding the row locks.
            analysis_data: List of dicts, each with 'fen', 'score', 'next_moves'.
        
        Returns:
            The number of rows updated.
        """
        if not analysis_data:
            print("No analysis data provided for update.", flush=True)
            return 0
            
        # We do not use a 'with' block here, as the session is managed by the caller
        try:
            prepared_data = []
            for item in analysis_data:
                prepared_data.append({
                    'p_fen': item['fen'],
                    'p_score': item['score'],
                    'p_next_moves': item['next_moves']
                })

            # --- THE CORE FIX ---
            # Use self.db_class.__table__ (Core) instead of self.db_class (ORM)
            # to ensure compatibility with bulk parameter binding.
            stmt = (
                update(self.db_class.__table__) 
                .where(self.db_class.fen == bindparam('p_fen'))
                .values(
                    score=bindparam('p_score'),
                    next_moves=bindparam('p_next_moves')
                )
            )
            
            result = await session.execute(
                stmt,
                prepared_data,
                execution_options={"synchronize_session": False}
            )
            
            # --- COSMETIC FIX for -1 rowcount ---
            total_updated_rows = len(analysis_data)
            
            print(f"Successfully staged {total_updated_rows} FEN updates for analysis data (pending commit).", flush=True)
            
            # DO NOT COMMIT HERE. The caller (run_analysis_job) will commit.
            return total_updated_rows
        except Exception as e:
            # DO NOT ROLL BACK HERE. The caller will roll back.
            print(f"An error occurred during bulk update of FEN analysis data: {e}", flush=True)
            raise
            
async def reset_all_game_fens_done_to_false() -> int:
    """
    Resets the 'fens_done' column to False for all Game records where it is currently True.
    """
    async with AsyncDBSession() as session:
        try:
            stmt = (
                update(Game)
                .where(Game.fens_done == True)
                .values(fens_done=False)
            )
            result = await session.execute(stmt)
            await session.commit()
            print(f"Successfully reset 'fens_done' to False for {result.rowcount} game(s).")
            return result.rowcount
        except Exception as e:
            await session.rollback()
            print(f"An error occurred while resetting 'fens_done' status: {e}")
            raise



### inner_api/database/engine.py 
# chessism_api/database/engine.py

import asyncio
from urllib.parse import urlparse
import asyncpg # For direct async DB operations
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker

# --- CORRECTED IMPORT ---
# This is the correct absolute import path based on your project structure.
from chessism_api.database.models import Base
# ---

async_engine = None
AsyncDBSession = sessionmaker(expire_on_commit=False, class_=AsyncSession)

async def init_db(connection_string: str):
    """
    Initializes the asynchronous SQLAlchemy database engine and ensures
    the database and all mapped tables exist.
    """
    global async_engine

    # Parse connection string for asyncpg (used for initial DB creation check)
    parsed_url = urlparse(connection_string)
    db_user = parsed_url.username
    db_password = parsed_url.password
    db_host = parsed_url.hostname
    db_port = parsed_url.port if parsed_url.port else 5432 # Default PostgreSQL port
    db_name = parsed_url.path.lstrip('/')

    temp_conn = None
    try:
        # Connect to a default database (e.g., 'postgres') to check/create the target database
        temp_conn = await asyncpg.connect(
            user=db_user,
            password=db_password,
            host=db_host,
            port=db_port,
            database='postgres' # Connect to a default database to perform creation
        )
        
        # Check if the target database exists
        db_exists_query = f"SELECT 1 FROM pg_database WHERE datname='{db_name}'"
        db_exists = await temp_conn.fetchval(db_exists_query)

        if not db_exists:
            print(f"Database '{db_name}' does not exist. Creating...")
            await temp_conn.execute(f'CREATE DATABASE "{db_name}"')
            print(f"Database '{db_name}' created.")
        else:
            print(f"Database '{db_name}' already exists.")

    except asyncpg.exceptions.DuplicateDatabaseError:
        print(f"Database '{db_name}' already exists (concurrent creation attempt).")
    except Exception as e:
        print(f"Error during database existence check/creation: {e}")
        # In a production env, you might want to retry or raise
        print("Continuing with engine creation...")
        pass # Allow SQLAlchemy to handle it if asyncpg fails
    finally:
        if temp_conn:
            await temp_conn.close() # Ensure the temporary connection is closed

    # Create the SQLAlchemy async engine for the actual application
    # Note: asyncpg connection string uses 'postgresql+asyncpg://', not just 'postgresql://'
    if not connection_string.startswith("postgresql+asyncpg://"):
        connection_string = connection_string.replace("postgresql://", "postgresql+asyncpg://", 1)

    async_engine = create_async_engine(connection_string, echo=False) # echo=True for SQL logging

    # Ensure database tables exist using the async engine
    async with async_engine.begin() as conn:
        print("Ensuring database tables exist...")
        # run_sync is used to execute synchronous metadata operations (like create_all)
        # within an async context
        await conn.run_sync(Base.metadata.create_all)
        print("Database tables checked/created.")
        
    # Configure the sessionmaker to use this async engine
    AsyncDBSession.configure(bind=async_engine)
    print("Asynchronous database initialization complete.")



### inner_api/database/ask_db.py 
# chessism_api/database/ask_db.py
# chessism_api/database/ask_db.py
import os
import asyncio
import time # <-- Added for internal timing in helpers
from typing import List, Dict, Any, Tuple, Set, Optional
from constants import CONN_STRING
from sqlalchemy.exc import ResourceClosedError
from sqlalchemy import text, select, update, func
from sqlalchemy.ext.asyncio import AsyncSession

# --- FIXED IMPORTS ---
from chessism_api.database.engine import async_engine, AsyncDBSession, init_db
from chessism_api.database.models import Fen, Game, AnalysisTimes, PlayerStats, game_fen_association
from chessism_api.database.db_interface import DBInterface
# ---

async def get_all_database_names():
    """
    Fetches the names of all databases accessible by the current connection.
    """
    if async_engine is None:
        print("Error: Database engine not initialized. Call init_db() first.")
        raise RuntimeError("Database engine not initialized. Call init_db() first.")

    dialect_name = async_engine.dialect.name
    query = ""
    
    if "postgresql" in dialect_name:
        query = "SELECT datname FROM pg_database WHERE datistemplate = false;"
    else:
        print(f"Warning: Database dialect '{dialect_name}' not explicitly handled.")
        return []

    print(f"Querying databases for {dialect_name} using: {query}")
    try:
        results = await open_async_request(query, fetch_as_dict=True)
        if results:
            db_names = []
            for row in results:
                if 'datname' in row:
                    db_names.append(row['datname'])
            return db_names
        return []
    except Exception as e:
        print(f"Error fetching database names: {e}")
        return []

async def open_async_request(sql_question: str,
                                 params: dict = None,
                                 fetch_as_dict: bool = False):
    """
    Executes an asynchronous SQL query, optionally with parameters, and fetches results.
    Uses AsyncDBSession for connection management.
    """
    async with AsyncDBSession() as session:
        try:
            if params:
                result = await session.execute(text(sql_question), params)
            else:
                result = await session.execute(text(sql_question))

            sql_upper = sql_question.strip().upper()
            if sql_upper.startswith("DROP") or \
               sql_upper.startswith("CREATE") or \
               sql_upper.startswith("ALTER") or \
               sql_upper.startswith("TRUNCATE"):
                
                print(f"DDL operation '{sql_question}' executed successfully.")
                await session.commit() # Explicitly commit DDL
                return None

            if fetch_as_dict:
                rows = result.fetchall()
                return [row._mapping for row in rows]
            else:
                return result.fetchall()
                
        except ResourceClosedError as e:
            sql_upper = sql_question.strip().upper()
            if not (sql_upper.startswith("SELECT") or sql_upper.startswith("WITH")):
                await session.commit() # Commit changes if it was a non-row-returning DML
                print(f"Non-row-returning statement executed: {sql_question}")
                return None
            print(f"Warning: Attempted to fetch rows from a non-row-returning statement: {sql_question}. Error: {e}")
            return None
        except Exception as e:
            await session.rollback() # Ensure rollback on error
            print(f"Error in open_async_request: {e}")
            raise

async def delete_all_leela_tables():
    """
    Deletes specified Leela-related tables asynchronously.
    """
    async with AsyncDBSession() as session:
        for table_name_to_delete in ['fen','game_fen_association']:
            print(f"Deleting table: {table_name_to_delete}...")
            try:
                await session.execute(text(f"DROP TABLE IF EXISTS \"{table_name_to_delete}\" CASCADE;"))
                await session.commit() # Commit after each drop
                print(f"Successfully deleted table: {table_name_to_delete}")
            except Exception as e:
                await session.rollback()
                print(f"An unexpected error occurred during deletion of {table_name_to_delete}: {e}")
    print("All specified Leela tables deletion attempt complete.")


async def get_players_with_names() -> List[Dict[str, Any]]:
    """
    Retrieves all player records where the 'name' column is not NULL,
    returning only their player_name.
    """
    sql_query = """
        SELECT
            player_name
        FROM
            player
        WHERE
            name IS NOT NULL;
    """
    result = await open_async_request(
        sql_query,
        fetch_as_dict=True
    )
    return result

async def reset_player_game_fens_done_to_false(player_name: str) -> int:
    """
    Resets the 'fens_done' column to False for all Game records associated with a specific player.
    """
    if not player_name:
        print("Player name cannot be empty. No games will be reset.")
        return 0

    async with AsyncDBSession() as session:
        try:
            stmt = (
                update(Game)
                .where(
                    (Game.white == player_name) | (Game.black == player_name)
                )
                .values(fens_done=False)
            )
            result = await session.execute(stmt)
            await session.commit()
            print(f"Successfully reset 'fens_done' to False for {result.rowcount} game(s) involving player '{player_name}'.")
            return result.rowcount

        except Exception as e:
            await session.rollback()
            print(f"An error occurred while resetting 'fens_done' status for player '{player_name}': {e}")
            raise


async def delete_analysis_times():
    """
    Deletes the analysis_times table asynchronously.
    """
    async with AsyncDBSession() as session:
        print(f"Deleting table: analysis_times ...")
        try:
            await session.execute(text(f"DROP TABLE IF EXISTS analysis_times CASCADE;"))
            await session.commit()
            print(f"Successfully deleted table: analysis_times")
        except Exception as e:
            await session.rollback()
            print(f"An unexpected error occurred during deletion of analysis_times: {e}")
    print("analysis_times table deletion attempt complete.")

async def save_analysis_times(batch_data):
    print('________')
    analysis_times_interface = DBInterface(AnalysisTimes)
    await analysis_times_interface.create(batch_data)
    print('_________')

async def get_games_already_in_db(links_to_check: Tuple[int, ...]) -> Set[int]:
    """
    Efficiently checks which game links already exist in the database
    using a temporary table for a large number of links.
    """
    if not links_to_check:
        return set()

    links_found_in_db = set()
    BATCH_SIZE = 1000
    
    async with AsyncDBSession() as session:
        start_time = time.time()
        try:
            # 1. Create a temporary table
            temp_table_name = "temp_game_links_check"
            await session.execute(text(f"""
                CREATE TEMPORARY TABLE IF NOT EXISTS {temp_table_name} (
                    link_col BIGINT PRIMARY KEY
                ) ON COMMIT DROP;
            """))

            # 2. Insert links into the temporary table in batches
            for i in range(0, len(links_to_check), BATCH_SIZE):
                batch = links_to_check[i : i + BATCH_SIZE]
                values_clause = ", ".join([f"({link})" for link in batch])
                
                if values_clause:
                    insert_sql = f"INSERT INTO {temp_table_name} (link_col) VALUES {values_clause} ON CONFLICT DO NOTHING;"
                    await session.execute(text(insert_sql))

            # 3. Join game table with the temporary table
            join_sql = f"""
                SELECT g.link 
                FROM game AS g
                JOIN {temp_table_name} AS t ON g.link = t.link_col;
            """
            result = await session.execute(text(join_sql))
            links_found_in_db.update(result.scalars().all())

            print(f"Checked {len(links_to_check)} links against DB in {time.time() - start_time:.2f}s. Found {len(links_found_in_db)} existing.")
        
        except Exception as e:
            await session.rollback()
            print(f"Error during temp table game check: {e}")
            raise
        
    return links_found_in_db

async def drop_player_stats_table():
    """
    Drops the 'player_stats' table to allow for schema recreation.
    """
    print("Attempting to drop 'player_stats' table...")
    try:
        await open_async_request("DROP TABLE IF EXISTS player_stats CASCADE;")
        print("Table 'player_stats' dropped successfully.")
    except Exception as e:
        print(f"Error dropping 'player_stats' table: {e}")

# --- NEW FUNCTION ---
async def get_top_fens(limit: int = 20) -> List[Dict[str, Any]]:
    """
    Retrieves the top N FENs based on the highest count of n_games.
    """
    sql_query = """
        SELECT 
            fen,
            n_games,
            moves_counter,
            score
        FROM 
            fen
        ORDER BY 
            n_games DESC
        LIMIT :limit;
    """
    params = {"limit": limit}
    
    # Use open_async_request to execute the query
    result = await open_async_request(
        sql_query,
        params=params,
        fetch_as_dict=True
    )
    return result

# --- NEW FUNCTION ---
async def get_top_fens_unscored(limit: int = 20) -> List[Dict[str, Any]]:
    """
    Retrieves the top N FENs based on the highest count of n_games,
    where the score has NOT been calculated yet.
    """
    sql_query = """
        SELECT 
            fen,
            n_games,
            score
        FROM 
            fen
        WHERE
            score IS NULL
        ORDER BY 
            n_games DESC
        LIMIT :limit;
    """
    params = {"limit": limit}
    
    # Use open_async_request to execute the query
    result = await open_async_request(
        sql_query,
        params=params,
        fetch_as_dict=True
    )
    return result

# --- NEW FUNCTION ---
async def get_sum_n_games(threshold: int = 10) -> Optional[int]:
    """
    Calculates the sum of all n_games where n_games > threshold.
    """
    async with AsyncDBSession() as session:
        try:
            stmt = (
                select(func.sum(Fen.n_games))
                .where(Fen.n_games > threshold)
            )
            result = await session.execute(stmt)
            total_sum = result.scalar()
            return total_sum
        except Exception as e:
            print(f"Error calculating sum of n_games: {e}")
            return None

# --- MODIFIED FUNCTION (FOR ANALYSIS JOB) ---
async def get_fens_for_analysis(limit: int) -> Tuple[Optional[AsyncSession], Optional[List[str]]]:
    """
    Fetches the next batch of FENs that need analysis.
    Starts a new transaction and applies a row-level lock.
    
    Returns the session (which holds the lock) and the list of FENs.
    The CALLER is responsible for committing or rolling back the session.
    """
    # Create a new session for this atomic operation
    session = AsyncDBSession()
    try:
        # Start the transaction
        await session.begin()
        
        stmt = (
            select(Fen.fen)
            .where(Fen.score.is_(None))
            .order_by(Fen.n_games.desc())
            .limit(limit)
            .with_for_update(skip_locked=True) # <-- The "Worker Queue" magic
        )
        
        result = await session.execute(stmt)
        fens = result.scalars().all()
        
        if not fens:
            # If no FENs, roll back and close the session immediately
            await session.rollback()
            await session.close()
            return None, None
            
        # Return the session *and* the fens. The session is still open and holds the lock.
        return session, fens

    except Exception as e:
        print(f"Error in get_fens_for_analysis: {repr(e)}", flush=True)
        # Rollback and close on error
        await session.rollback()
        await session.close()
        return None, None

# --- MODIFIED FUNCTION (FOR PLAYER ANALYSIS JOB) ---
async def get_player_fens_for_analysis(
    player_name: str,
    limit: int
) -> Tuple[Optional[AsyncSession], Optional[List[str]]]:
    """
    Fetches the next batch of FENs for a specific player.
    Starts a new transaction and applies a row-level lock.
    
    Returns the session (which holds the lock) and the list of FENs.
    """
    # Create a new session for this atomic operation
    session = AsyncDBSession()
    try:
        await session.begin()
        
        # This is a complex join to find FENs -> from games -> where player matches
        stmt = (
            select(Fen.fen)
            .join(game_fen_association, Fen.fen == game_fen_association.c.fen_fen)
            .join(Game, game_fen_association.c.game_link == Game.link)
            .where(
                (Game.white == player_name) | (Game.black == player_name)
            )
            .where(Fen.score.is_(None))
            .order_by(Fen.n_games.desc())
            .limit(limit)
            .with_for_update(skip_locked=True)
        )
        
        result = await session.execute(stmt)
        fens = result.scalars().all()
        
        if not fens:
            await session.rollback()
            await session.close()
            return None, None
            
        return session, fens

    except Exception as e:
        print(f"Error in get_player_fens_for_analysis: {repr(e)}", flush=True)
        await session.rollback()
        await session.close()
        return None, None

