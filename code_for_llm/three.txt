
            PART THREE: CHESSISM_API ROUTERS
            This is the third part, it contains some general helpers for the api

            


### chessism_api/redis_client.py 
# chessism_api/redis_client.py
import os
from arq import create_pool
from arq.connections import ArqRedis, RedisSettings

REDIS_HOST = os.environ.get("REDIS_HOST", "localhost")
redis_settings = RedisSettings(host=REDIS_HOST, port=6379)

# This will be our shared client pool
redis_pool: ArqRedis = None

async def get_redis_pool() -> ArqRedis:
    """
    Returns the shared ArqRedis client pool.
    """
    global redis_pool
    if not redis_pool:
        redis_pool = await create_pool(redis_settings)
    return redis_pool

async def close_redis_pool():
    """
    Closes the shared client pool.
    """
    if redis_pool:
        await redis_pool.close()



### chessism_api/routers/games.py 
# chessism_api/routers/games.py

# chessism_api/routers/games.py

from fastapi.responses import JSONResponse
from fastapi import APIRouter, Body, HTTPException # <-- Added HTTPException
from typing import Dict, Any # <-- Added typing

# --- FIXED IMPORTS ---
from chessism_api.operations.games import create_games, read_game, update_player_games
from chessism_api.database.ask_db import open_async_request # <-- Fixed import
# ---

router = APIRouter()


@router.get("/{link}") # --- FIX: Removed '/games' prefix ---
async def api_read_game(link: str) -> JSONResponse:
    """
    Retrieves game information by its link.
    """
    print(f'api call for link: {link}')
    try:
        game = await read_game(link)
        if not game: # Check if game list is empty
            raise HTTPException(status_code=404, detail=f"Game with link '{link}' not found.")
        # Return the first game found (links should be unique)
        return JSONResponse(content=game[0])
    except Exception as e:
        print(f"Error fetching game {link}: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")

@router.post("") # --- FIX: Changed route from "/" to "" ---
async def api_create_game(data: Dict[str, Any] = Body(...)) -> JSONResponse: # <-- Use Dict
    """
    data = {"player_name": "some_player_name"}
    
    Fetches every available game for the player_name
    formats them and inserts them into DB.
    """
    try:
        player_name = data["player_name"]
    except KeyError:
        raise HTTPException(status_code=400, detail="Payload must include 'player_name'.")
        
    congratulation = await create_games(data)
    return JSONResponse(content={"message": congratulation})


# --- NEW ENDPOINT ---
@router.post("/update")
async def api_update_player_games(data: Dict[str, Any] = Body(...)) -> JSONResponse:
    """
    data = {"player_name": "some_player_name"}
    
    Fetches games from the last recorded month to the present.
    """
    try:
        player_name = data["player_name"]
    except KeyError:
        raise HTTPException(status_code=400, detail="Payload must include 'player_name'.")
        
    message = await update_player_games(data)
    return JSONResponse(content={"message": message})



### chessism_api/routers/fens.py 
#chessism_api/routers/fens.py

# --- MODIFIED: Imports ---
from fastapi import APIRouter, Body, HTTPException, Depends
from fastapi.responses import JSONResponse
from arq.connections import ArqRedis
import math # <-- Import math

# --- Import the job function (for reference) and redis client ---
# --- MODIFIED: Import the "boss" job ---
from chessism_api.operations.fens import run_fen_pipeline
from chessism_api.redis_client import get_redis_pool
# ---

from chessism_api.database.ask_db import (
    get_top_fens, 
    get_sum_n_games, 
    get_top_fens_unscored
)
from typing import Dict, Any 

router = APIRouter()

# --- THIS IS THE FIX ---
# This endpoint now enqueues ONE "boss" job
@router.post("/generate")
async def api_generate_fens(
    data: Dict[str, int] = Body(...),
    redis: ArqRedis = Depends(get_redis_pool)
):
    """
    Triggers the FEN generation pipeline.
    This enqueues a single 'boss' job which then coordinates
    the 3 parallel 'fen-worker' jobs.
    
    Payload: {"total_games_to_process": 400000, "batch_size": 1000}
    """
    try:
        total_games = data.get("total_games_to_process", 1000000)
        batch_size = data.get("batch_size", 1000)
        num_workers = 3
    except Exception:
        raise HTTPException(status_code=400, detail="Invalid payload format.")
    
    print(f"Enqueuing FEN Pipeline job for {total_games} games.")
    
    # Enqueue 3 separate jobs
    await redis.enqueue_job(
        'run_fen_pipeline', # The "boss" job
        total_games_to_process=total_games,
        batch_size=batch_size,
        num_workers=num_workers,
        _queue_name='pipeline_queue' # Send to the pipeline worker
    )
    
    return JSONResponse(
        status_code=202, # Accepted
        content={"message": f"FEN Generation Pipeline started for {total_games} total games."}
    )
# --- END FIX ---


# --- (The rest of the router file is unchanged) ---
@router.get("/top")
# ... (existing code, unchanged) ...
async def api_get_sum_n_games(threshold: int = 10) -> JSONResponse:
# ... (existing code, unchanged) ...
    return JSONResponse(content={
        "threshold": threshold,
        "total_sum_n_games": total_sum
    })



### chessism_api/routers/players.py 
# chessism_api/routers/players.py

from fastapi import APIRouter, HTTPException, Body, BackgroundTasks
from fastapi.responses import JSONResponse
from typing import Dict, Any
from chessism_api.database.ask_db import get_player_fen_score_counts
# --- UPDATED IMPORTS ---
from chessism_api.operations.players import (
    get_current_players_with_games_in_db, 
    read_player, 
    insert_player,
    create_and_store_player_stats,
    read_player_stats,
    update_stats_for_all_primary_players
)
# ---

router = APIRouter()

@router.get("/{player_name}/fen_counts")
async def api_get_player_fen_counts(player_name: str) -> JSONResponse:
    """
    Returns the count of FENs with score 0, score != 0, and unscored (NULL) for a player.
    """
    player_name_lower = player_name.lower()
    counts = await get_player_fen_score_counts(player_name_lower)
    return JSONResponse(content=counts)

@router.get("/current_players")
async def api_get_current_players_with_games():
    """
    Fetches all players that have a full profile (joined != 0).
    """
    result = await get_current_players_with_games_in_db()
    return JSONResponse(content=result)

# --- RE-ORDERED: Specific routes first ---

@router.post("/update-all-stats")
async def api_update_all_stats(background_tasks: BackgroundTasks):
    """
    Triggers a long-running background task to update the stats
    for EVERY primary player in the database.
    
    Responds immediately with a "Job Started" message.
    """
    print("Received request to update all player stats.")
    background_tasks.add_task(update_stats_for_all_primary_players)
    return JSONResponse(
        status_code=202, # Accepted
        content={"message": "Batch job started: Updating stats for all primary players in the background."}
    )

@router.get("/{player_name}/stats")
async def api_get_player_stats(player_name: str) -> JSONResponse:
    """
    Fetches a player's stats from Chess.com and updates the local database.
    1. Always attempts to fetch fresh data from the Chess.com API.
    2. Saves the data to the DB (inserting or updating).
    3. Returns the fresh data.
    """
    player_name_lower = player_name.lower()
    
    print(f"Fetching fresh stats for {player_name_lower} from Chess.com...")
    try:
        # This function handles the full "fetch and upsert" logic
        new_stats_data = await create_and_store_player_stats(player_name_lower)
        
        if new_stats_data:
            print(f"Successfully fetched and upserted stats for {player_name_lower}.")
            # .model_dump() converts the Pydantic object to a dict
            return JSONResponse(content=new_stats_data.model_dump())
        else:
            # This means get_player_stats() returned None
            raise HTTPException(status_code=404, detail="Stats not found on Chess.com (or connection failed).")
            
    except Exception as e:
        print(f"Error during stats fetch for {player_name_lower}: {repr(e)}")
        raise HTTPException(status_code=500, detail="An internal error occurred while fetching stats.")


# --- RE-ORDERED: General route last ---

@router.get("/{player_name}")
async def api_get_player_profile(player_name: str) -> JSONResponse:
    """
    Fetches a player's profile.
    1. Tries to read from the local database.
    2. If not found, attempts to fetch from Chess.com and save.
    """
    # 1. Try to read from the database first
    player_data = await read_player(player_name)
    
    if player_data:
        print(f"Found player {player_name} in database.")
        return JSONResponse(content=player_data)

    # 2. If not in DB, try to fetch from Chess.com (which also saves it)
    print(f"Player {player_name} not in DB. Fetching from Chess.com...")
    try:
        new_player_data = await insert_player({"player_name": player_name})
        
        if new_player_data:
            print(f"Successfully fetched and saved {player_name}.")
            # .model_dump() converts the Pydantic object to a dict for the JSON response
            return JSONResponse(content=new_player_data.model_dump())
        else:
            # This means get_profile() returned None (e.g., ConnectTimeout or 404 from Chess.com)
            raise HTTPException(status_code=404, detail="Player not found in database or on Chess.com (or connection failed).")
            
    except Exception as e:
        print(f"Error during insert_player fetch for {player_name}: {repr(e)}")
        raise HTTPException(status_code=500, detail="An internal error occurred while fetching the player.")



### chessism_api/routers/stats.py 
# chessism_api/routers/stats.py
from fastapi import APIRouter

router = APIRouter()

# --- TODO: Add stats endpoints here (e.g., /global, /player/{player_name}) ---
# (Leaving empty as per our plan)



### chessism_api/routers/analysis.py 
# chessism_api/routers/analysis.py

# --- REMOVED BackgroundTasks, Added Depends ---
from fastapi import APIRouter, Body, HTTPException, Depends
from fastapi.responses import JSONResponse
from typing import Dict, Any
from arq.connections import ArqRedis
import math # <-- NEW: Import math for ceiling

# --- Import the job functions (they are now just for reference) ---
from chessism_api.operations.analysis import (
    run_analysis_job,
    run_player_analysis_job
)
# --- NEW: Import the Redis client ---
from chessism_api.redis_client import get_redis_pool

router = APIRouter()

# --- Define queue names ---
QUEUE_NAMES = {
    0: "gpu_0_queue",
    1: "gpu_1_queue"
}

@router.post("/run_job")
async def api_run_analysis_job(
    data: Dict[str, Any] = Body(...),
    redis: ArqRedis = Depends(get_redis_pool) # <-- NEW: Get Redis client
):
    """
    Enqueues a job to analyze FENs from the main database pool.
    
    Payload:
    {
        "gpu_index": 0,
        "total_fens_to_process": 100000,
        "batch_size": 100,
        "nodes_limit": 50000
    }
    """
    try:
        gpu_index = int(data["gpu_index"])
        total_fens = int(data.get("total_fens_to_process", 1000000))
        batch_size = int(data.get("batch_size", 100))
        nodes_limit = int(data.get("nodes_limit", 50000))
    except (KeyError, ValueError, TypeError):
        raise HTTPException(status_code=400, detail="Invalid payload. Required: 'gpu_index' (int).")

    if gpu_index not in [0, 1]:
        raise HTTPException(status_code=400, detail="'gpu_index' must be 0 or 1.")

    queue_name = QUEUE_NAMES[gpu_index]

    # --- NEW: Calculate dynamic timeout based on your formula ---
    # (total_fens * 2.1)
    # We add a 600-second (10 min) buffer for safety and DB operations.
    calculated_timeout = math.ceil(total_fens * 2.1) + 600
    
    print(f"Enqueuing analysis job on {queue_name}. Target: {total_fens} FENs. Calculated timeout: {calculated_timeout}s")
    
    # --- NEW: Enqueue the job instead of running it ---
    await redis.enqueue_job(
        'run_analysis_job', # This must match the function name in worker.py
        gpu_index=gpu_index,
        total_fens_to_process=total_fens,
        batch_size=batch_size,
        nodes_limit=nodes_limit,
        _queue_name=queue_name, # Tell arq which queue to use
        _job_timeout=calculated_timeout # <-- THIS IS THE FIX
    )
    
    return JSONResponse(
        status_code=202, # Accepted
        content={
            "message": f"Batch analysis job enqueued on {queue_name}.",
            "gpu_index": gpu_index,
            "total_fens_to_process": total_fens,
        }
    )

@router.post("/run_player_job")
async def api_run_player_analysis_job(
    data: Dict[str, Any] = Body(...),
    redis: ArqRedis = Depends(get_redis_pool) # <-- NEW: Get Redis client
):
    """
    Enqueues a job to analyze FENs for a specific player.
    
    Payload:
    {
        "player_name": "hikaru",
        "gpu_index": 0,
        "total_fens_to_process": 1000,
        "batch_size": 50,
        "nodes_limit": 50000
    }
    """
    try:
        player_name = str(data["player_name"]).lower()
        gpu_index = int(data["gpu_index"])
        total_fens = int(data.get("total_fens_to_process", 100000))
        batch_size = int(data.get("batch_size", 50))
        nodes_limit = int(data.get("nodes_limit", 50000))
    except (KeyError, ValueError, TypeError):
        raise HTTPException(status_code=400, detail="Invalid payload. Required: 'player_name' (str) and 'gpu_index' (int).")

    if gpu_index not in [0, 1]:
        raise HTTPException(status_code=400, detail="'gpu_index' must be 0 or 1.")

    queue_name = QUEUE_NAMES[gpu_index]

    # --- NEW: Calculate dynamic timeout based on your formula ---
    # (total_fens * 2.1)
    # We add a 600-second (10 min) buffer for safety and DB operations.
    calculated_timeout = math.ceil(total_fens * 2.1) + 600

    print(f"Enqueuing PLAYER analysis job for '{player_name}' on {queue_name}. Calculated timeout: {calculated_timeout}s")
    
    # --- NEW: Enqueue the job ---
    await redis.enqueue_job(
        'run_player_analysis_job', # Function name
        player_name=player_name,
        gpu_index=gpu_index,
        total_fens_to_process=total_fens,
        batch_size=batch_size,
        nodes_limit=nodes_limit,
        _queue_name=queue_name, # The specific queue to use
        _job_timeout=calculated_timeout # <-- THIS IS THE FIX
    )
    
    return JSONResponse(
        status_code=202, # Accepted
        content={
            "message": f"Batch player analysis job for '{player_name}' enqueued on {queue_name}.",
            "player_name": player_name,
            "gpu_index": gpu_index,
        }
    )

